{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67bbcefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARPABET PHONEME MAPPING\n",
    "# DO NOT CHANGE\n",
    "# This overwrites the phonetics.py file.\n",
    "\n",
    "CMUdict_ARPAbet = {\n",
    "    \"\" : \" \", # BLANK TOKEN\n",
    "    \"[SIL]\": \"-\", \"NG\": \"G\", \"F\" : \"f\", \"M\" : \"m\", \"AE\": \"@\", \n",
    "    \"R\"    : \"r\", \"UW\": \"u\", \"N\" : \"n\", \"IY\": \"i\", \"AW\": \"W\", \n",
    "    \"V\"    : \"v\", \"UH\": \"U\", \"OW\": \"o\", \"AA\": \"a\", \"ER\": \"R\", \n",
    "    \"HH\"   : \"h\", \"Z\" : \"z\", \"K\" : \"k\", \"CH\": \"C\", \"W\" : \"w\", \n",
    "    \"EY\"   : \"e\", \"ZH\": \"Z\", \"T\" : \"t\", \"EH\": \"E\", \"Y\" : \"y\", \n",
    "    \"AH\"   : \"A\", \"B\" : \"b\", \"P\" : \"p\", \"TH\": \"T\", \"DH\": \"D\", \n",
    "    \"AO\"   : \"c\", \"G\" : \"g\", \"L\" : \"l\", \"JH\": \"j\", \"OY\": \"O\", \n",
    "    \"SH\"   : \"S\", \"D\" : \"d\", \"AY\": \"Y\", \"S\" : \"s\", \"IH\": \"I\",\n",
    "    \"[SOS]\": \"[SOS]\", \"[EOS]\": \"[EOS]\"}\n",
    "\n",
    "CMUdict = list(CMUdict_ARPAbet.keys())\n",
    "ARPAbet = list(CMUdict_ARPAbet.values())\n",
    "\n",
    "\n",
    "PHONEMES = CMUdict\n",
    "mapping = CMUdict_ARPAbet\n",
    "LABELS = ARPAbet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b1cff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [0,1,1,2,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65172ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\n",
    "\n",
    "for label in test:\n",
    "    string += LABELS[label]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b69d132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' --GGf'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(list(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a613bcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (998,\"IkltcWuODutujGjR@uaeOR@WWedt@bvUhUEksvYvpUkRewniTniIsSUeYcDd@wAWdEYtmeAdDOpO\")\n",
    "b = (999,\"RECsgfEDcAjlURmwynAabWDIIUnSTkiocjGoeIhvipwwWUUmGnriOrIDGyIefria@siRWdfDcwUs\")\n",
    "c = (1000,\"aUAmcnwbnhrtRcriI@k@Ovg-a-@bbtpkRDnmRtlarTIvDugWWmfaRidafekhgyreodny-Yej-Ruv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a76558c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4edfd33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd5ae421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "77b69076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "m = nn.LSTM(128, 108, num_layers=3, bidirectional= True, dropout=0.25, batch_first= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "26a24bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(32, 304, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "746d3e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e948f6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 304, 216])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0f1e3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 32, 256])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "18c34357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 32, 256])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4339302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "a = [2,1,1,2,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "514d79be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(a)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4bf6e7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "hidden = np.zeros((2, 32, 518))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "41f84598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 518)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "42780824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 32, 518)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([[hidden[0]], [hidden[1]]], axis = 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6dfb396c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 518)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ab2a65ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(2, -1, -1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8c4e28ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fae7e9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.transforms as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8e26393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Prepare data and utility functions. {display-mode: \"form\"}\n",
    "#@markdown\n",
    "#@markdown You do not need to look into this cell.\n",
    "#@markdown Just execute once and you are good to go.\n",
    "#@markdown\n",
    "#@markdown In this tutorial, we will use a speech data from [VOiCES dataset](https://iqtlabs.github.io/voices/), which is licensed under Creative Commos BY 4.0.\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Preparation of data and helper functions.\n",
    "#-------------------------------------------------------------------------------\n",
    "import io\n",
    "import os\n",
    "import math\n",
    "import tarfile\n",
    "import multiprocessing\n",
    "\n",
    "import scipy\n",
    "import librosa\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "import requests\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "[width, height] = matplotlib.rcParams['figure.figsize']\n",
    "if width < 10:\n",
    "  matplotlib.rcParams['figure.figsize'] = [width * 2.5, height]\n",
    "\n",
    "_SAMPLE_DIR = \"_sample_data\"\n",
    "SAMPLE_WAV_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/steam-train-whistle-daniel_simon.wav\"\n",
    "SAMPLE_WAV_PATH = os.path.join(_SAMPLE_DIR, \"steam.wav\")\n",
    "\n",
    "SAMPLE_WAV_SPEECH_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
    "SAMPLE_WAV_SPEECH_PATH = os.path.join(_SAMPLE_DIR, \"speech.wav\")\n",
    "\n",
    "SAMPLE_RIR_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/room-response/rm1/impulse/Lab41-SRI-VOiCES-rm1-impulse-mc01-stu-clo.wav\"\n",
    "SAMPLE_RIR_PATH = os.path.join(_SAMPLE_DIR, \"rir.wav\")\n",
    "\n",
    "SAMPLE_NOISE_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/distractors/rm1/babb/Lab41-SRI-VOiCES-rm1-babb-mc01-stu-clo.wav\"\n",
    "SAMPLE_NOISE_PATH = os.path.join(_SAMPLE_DIR, \"bg.wav\")\n",
    "\n",
    "SAMPLE_MP3_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/steam-train-whistle-daniel_simon.mp3\"\n",
    "SAMPLE_MP3_PATH = os.path.join(_SAMPLE_DIR, \"steam.mp3\")\n",
    "\n",
    "SAMPLE_GSM_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/steam-train-whistle-daniel_simon.gsm\"\n",
    "SAMPLE_GSM_PATH = os.path.join(_SAMPLE_DIR, \"steam.gsm\")\n",
    "\n",
    "SAMPLE_TAR_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit.tar.gz\"\n",
    "SAMPLE_TAR_PATH = os.path.join(_SAMPLE_DIR, \"sample.tar.gz\")\n",
    "SAMPLE_TAR_ITEM = \"VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
    "\n",
    "S3_BUCKET = \"pytorch-tutorial-assets\"\n",
    "S3_KEY = \"VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
    "\n",
    "YESNO_DATASET_PATH = os.path.join(_SAMPLE_DIR, \"yes_no\")\n",
    "os.makedirs(YESNO_DATASET_PATH, exist_ok=True)\n",
    "os.makedirs(_SAMPLE_DIR, exist_ok=True)\n",
    "\n",
    "def _fetch_data():\n",
    "  uri = [\n",
    "    (SAMPLE_WAV_URL, SAMPLE_WAV_PATH),\n",
    "    (SAMPLE_WAV_SPEECH_URL, SAMPLE_WAV_SPEECH_PATH),\n",
    "    (SAMPLE_RIR_URL, SAMPLE_RIR_PATH),\n",
    "    (SAMPLE_NOISE_URL, SAMPLE_NOISE_PATH),\n",
    "    (SAMPLE_MP3_URL, SAMPLE_MP3_PATH),\n",
    "    (SAMPLE_GSM_URL, SAMPLE_GSM_PATH),\n",
    "    (SAMPLE_TAR_URL, SAMPLE_TAR_PATH),\n",
    "  ]\n",
    "  for url, path in uri:\n",
    "    with open(path, 'wb') as file_:\n",
    "      file_.write(requests.get(url).content)\n",
    "\n",
    "_fetch_data()\n",
    "\n",
    "def _download_yesno():\n",
    "  if os.path.exists(os.path.join(YESNO_DATASET_PATH, \"waves_yesno.tar.gz\")):\n",
    "    return\n",
    "  torchaudio.datasets.YESNO(root=YESNO_DATASET_PATH, download=True)\n",
    "\n",
    "YESNO_DOWNLOAD_PROCESS = multiprocessing.Process(target=_download_yesno)\n",
    "YESNO_DOWNLOAD_PROCESS.start()\n",
    "\n",
    "def _get_sample(path, resample=None):\n",
    "  effects = [\n",
    "    [\"remix\", \"1\"]\n",
    "  ]\n",
    "  if resample:\n",
    "    effects.extend([\n",
    "      [\"lowpass\", f\"{resample // 2}\"],\n",
    "      [\"rate\", f'{resample}'],\n",
    "    ])\n",
    "  return torchaudio.sox_effects.apply_effects_file(path, effects=effects)\n",
    "\n",
    "def get_speech_sample(*, resample=None):\n",
    "  return _get_sample(SAMPLE_WAV_SPEECH_PATH, resample=resample)\n",
    "\n",
    "def get_sample(*, resample=None):\n",
    "  return _get_sample(SAMPLE_WAV_PATH, resample=resample)\n",
    "\n",
    "def get_rir_sample(*, resample=None, processed=False):\n",
    "  rir_raw, sample_rate = _get_sample(SAMPLE_RIR_PATH, resample=resample)\n",
    "  if not processed:\n",
    "    return rir_raw, sample_rate\n",
    "  rir = rir_raw[:, int(sample_rate*1.01):int(sample_rate*1.3)]\n",
    "  rir = rir / torch.norm(rir, p=2)\n",
    "  rir = torch.flip(rir, [1])\n",
    "  return rir, sample_rate\n",
    "\n",
    "def get_noise_sample(*, resample=None):\n",
    "  return _get_sample(SAMPLE_NOISE_PATH, resample=resample)\n",
    "\n",
    "def print_stats(waveform, sample_rate=None, src=None):\n",
    "  if src:\n",
    "    print(\"-\" * 10)\n",
    "    print(\"Source:\", src)\n",
    "    print(\"-\" * 10)\n",
    "  if sample_rate:\n",
    "    print(\"Sample Rate:\", sample_rate)\n",
    "  print(\"Shape:\", tuple(waveform.shape))\n",
    "  print(\"Dtype:\", waveform.dtype)\n",
    "  print(f\" - Max:     {waveform.max().item():6.3f}\")\n",
    "  print(f\" - Min:     {waveform.min().item():6.3f}\")\n",
    "  print(f\" - Mean:    {waveform.mean().item():6.3f}\")\n",
    "  print(f\" - Std Dev: {waveform.std().item():6.3f}\")\n",
    "  print()\n",
    "  print(waveform)\n",
    "  print()\n",
    "\n",
    "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
    "  waveform = waveform.numpy()\n",
    "\n",
    "  num_channels, num_frames = waveform.shape\n",
    "  time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "  figure, axes = plt.subplots(num_channels, 1)\n",
    "  if num_channels == 1:\n",
    "    axes = [axes]\n",
    "  for c in range(num_channels):\n",
    "    axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
    "    axes[c].grid(True)\n",
    "    if num_channels > 1:\n",
    "      axes[c].set_ylabel(f'Channel {c+1}')\n",
    "    if xlim:\n",
    "      axes[c].set_xlim(xlim)\n",
    "    if ylim:\n",
    "      axes[c].set_ylim(ylim)\n",
    "  figure.suptitle(title)\n",
    "  plt.show(block=False)\n",
    "\n",
    "def plot_specgram(waveform, sample_rate, title=\"Spectrogram\", xlim=None):\n",
    "  waveform = waveform.numpy()\n",
    "\n",
    "  num_channels, num_frames = waveform.shape\n",
    "  time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "  figure, axes = plt.subplots(num_channels, 1)\n",
    "  if num_channels == 1:\n",
    "    axes = [axes]\n",
    "  for c in range(num_channels):\n",
    "    axes[c].specgram(waveform[c], Fs=sample_rate)\n",
    "    if num_channels > 1:\n",
    "      axes[c].set_ylabel(f'Channel {c+1}')\n",
    "    if xlim:\n",
    "      axes[c].set_xlim(xlim)\n",
    "  figure.suptitle(title)\n",
    "  plt.show(block=False)\n",
    "\n",
    "def play_audio(waveform, sample_rate):\n",
    "  waveform = waveform.numpy()\n",
    "\n",
    "  num_channels, num_frames = waveform.shape\n",
    "  if num_channels == 1:\n",
    "    display(Audio(waveform[0], rate=sample_rate))\n",
    "  elif num_channels == 2:\n",
    "    display(Audio((waveform[0], waveform[1]), rate=sample_rate))\n",
    "  else:\n",
    "    raise ValueError(\"Waveform with more than 2 channels are not supported.\")\n",
    "\n",
    "def inspect_file(path):\n",
    "  print(\"-\" * 10)\n",
    "  print(\"Source:\", path)\n",
    "  print(\"-\" * 10)\n",
    "  print(f\" - File size: {os.path.getsize(path)} bytes\")\n",
    "  print(f\" - {torchaudio.info(path)}\")\n",
    "\n",
    "def plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n",
    "  fig, axs = plt.subplots(1, 1)\n",
    "  axs.set_title(title or 'Spectrogram (db)')\n",
    "  axs.set_ylabel(ylabel)\n",
    "  axs.set_xlabel('frame')\n",
    "  im = axs.imshow(librosa.power_to_db(spec), origin='lower', aspect=aspect)\n",
    "  if xmax:\n",
    "    axs.set_xlim((0, xmax))\n",
    "  fig.colorbar(im, ax=axs)\n",
    "  plt.show(block=False)\n",
    "\n",
    "def plot_mel_fbank(fbank, title=None):\n",
    "  fig, axs = plt.subplots(1, 1)\n",
    "  axs.set_title(title or 'Filter bank')\n",
    "  axs.imshow(fbank, aspect='auto')\n",
    "  axs.set_ylabel('frequency bin')\n",
    "  axs.set_xlabel('mel bin')\n",
    "  plt.show(block=False)\n",
    "\n",
    "def get_spectrogram(\n",
    "    n_fft = 400,\n",
    "    win_len = None,\n",
    "    hop_len = None,\n",
    "    power = 2.0,\n",
    "):\n",
    "  waveform, _ = get_speech_sample()\n",
    "  spectrogram = T.Spectrogram(\n",
    "      n_fft=n_fft,\n",
    "      win_length=win_len,\n",
    "      hop_length=hop_len,\n",
    "      center=True,\n",
    "      pad_mode=\"reflect\",\n",
    "      power=power,\n",
    "  )\n",
    "  return spectrogram(waveform)\n",
    "\n",
    "def plot_pitch(waveform, sample_rate, pitch):\n",
    "  figure, axis = plt.subplots(1, 1)\n",
    "  axis.set_title(\"Pitch Feature\")\n",
    "  axis.grid(True)\n",
    "\n",
    "  end_time = waveform.shape[1] / sample_rate\n",
    "  time_axis = torch.linspace(0, end_time,  waveform.shape[1])\n",
    "  axis.plot(time_axis, waveform[0], linewidth=1, color='gray', alpha=0.3)\n",
    "\n",
    "  axis2 = axis.twinx()\n",
    "  time_axis = torch.linspace(0, end_time, pitch.shape[1])\n",
    "  ln2 = axis2.plot(\n",
    "      time_axis, pitch[0], linewidth=2, label='Pitch', color='green')\n",
    "\n",
    "  axis2.legend(loc=0)\n",
    "  plt.show(block=False)\n",
    "\n",
    "def plot_kaldi_pitch(waveform, sample_rate, pitch, nfcc):\n",
    "  figure, axis = plt.subplots(1, 1)\n",
    "  axis.set_title(\"Kaldi Pitch Feature\")\n",
    "  axis.grid(True)\n",
    "\n",
    "  end_time = waveform.shape[1] / sample_rate\n",
    "  time_axis = torch.linspace(0, end_time,  waveform.shape[1])\n",
    "  axis.plot(time_axis, waveform[0], linewidth=1, color='gray', alpha=0.3)\n",
    "\n",
    "  time_axis = torch.linspace(0, end_time, pitch.shape[1])\n",
    "  ln1 = axis.plot(time_axis, pitch[0], linewidth=2, label='Pitch', color='green')\n",
    "  axis.set_ylim((-1.3, 1.3))\n",
    "\n",
    "  axis2 = axis.twinx()\n",
    "  time_axis = torch.linspace(0, end_time, nfcc.shape[1])\n",
    "  ln2 = axis2.plot(\n",
    "      time_axis, nfcc[0], linewidth=2, label='NFCC', color='blue', linestyle='--')\n",
    "\n",
    "  lns = ln1 + ln2\n",
    "  labels = [l.get_label() for l in lns]\n",
    "  axis.legend(lns, labels, loc=0)\n",
    "  plt.show(block=False)\n",
    "\n",
    "DEFAULT_OFFSET = 201\n",
    "SWEEP_MAX_SAMPLE_RATE = 48000\n",
    "DEFAULT_LOWPASS_FILTER_WIDTH = 6\n",
    "DEFAULT_ROLLOFF = 0.99\n",
    "DEFAULT_RESAMPLING_METHOD = 'sinc_interpolation'\n",
    "\n",
    "def _get_log_freq(sample_rate, max_sweep_rate, offset):\n",
    "  \"\"\"Get freqs evenly spaced out in log-scale, between [0, max_sweep_rate // 2]\n",
    "\n",
    "  offset is used to avoid negative infinity `log(offset + x)`.\n",
    "\n",
    "  \"\"\"\n",
    "  half = sample_rate // 2\n",
    "  start, stop = math.log(offset), math.log(offset + max_sweep_rate // 2)\n",
    "  return torch.exp(torch.linspace(start, stop, sample_rate, dtype=torch.double)) - offset\n",
    "\n",
    "def _get_inverse_log_freq(freq, sample_rate, offset):\n",
    "  \"\"\"Find the time where the given frequency is given by _get_log_freq\"\"\"\n",
    "  half = sample_rate // 2\n",
    "  return sample_rate * (math.log(1 + freq / offset) / math.log(1 + half / offset))\n",
    "\n",
    "def _get_freq_ticks(sample_rate, offset, f_max):\n",
    "  # Given the original sample rate used for generating the sweep,\n",
    "  # find the x-axis value where the log-scale major frequency values fall in\n",
    "  time, freq = [], []\n",
    "  for exp in range(2, 5):\n",
    "    for v in range(1, 10):\n",
    "      f = v * 10 ** exp\n",
    "      if f < sample_rate // 2:\n",
    "        t = _get_inverse_log_freq(f, sample_rate, offset) / sample_rate\n",
    "        time.append(t)\n",
    "        freq.append(f)\n",
    "  t_max = _get_inverse_log_freq(f_max, sample_rate, offset) / sample_rate\n",
    "  time.append(t_max)\n",
    "  freq.append(f_max)\n",
    "  return time, freq\n",
    "\n",
    "def plot_sweep(waveform, sample_rate, title, max_sweep_rate=SWEEP_MAX_SAMPLE_RATE, offset=DEFAULT_OFFSET):\n",
    "  x_ticks = [100, 500, 1000, 5000, 10000, 20000, max_sweep_rate // 2]\n",
    "  y_ticks = [1000, 5000, 10000, 20000, sample_rate//2]\n",
    "\n",
    "  time, freq = _get_freq_ticks(max_sweep_rate, offset, sample_rate // 2)\n",
    "  freq_x = [f if f in x_ticks and f <= max_sweep_rate // 2 else None for f in freq]\n",
    "  freq_y = [f for f in freq if f >= 1000 and f in y_ticks and f <= sample_rate // 2]\n",
    "\n",
    "  figure, axis = plt.subplots(1, 1)\n",
    "  axis.specgram(waveform[0].numpy(), Fs=sample_rate)\n",
    "  plt.xticks(time, freq_x)\n",
    "  plt.yticks(freq_y, freq_y)\n",
    "  axis.set_xlabel('Original Signal Frequency (Hz, log scale)')\n",
    "  axis.set_ylabel('Waveform Frequency (Hz)')\n",
    "  axis.xaxis.grid(True, alpha=0.67)\n",
    "  axis.yaxis.grid(True, alpha=0.67)\n",
    "  figure.suptitle(f'{title} (sample rate: {sample_rate} Hz)')\n",
    "  plt.show(block=True)\n",
    "\n",
    "def get_sine_sweep(sample_rate, offset=DEFAULT_OFFSET):\n",
    "    max_sweep_rate = sample_rate\n",
    "    freq = _get_log_freq(sample_rate, max_sweep_rate, offset)\n",
    "    delta = 2 * math.pi * freq / sample_rate\n",
    "    cummulative = torch.cumsum(delta, dim=0)\n",
    "    signal = torch.sin(cummulative).unsqueeze(dim=0)\n",
    "    return signal\n",
    "\n",
    "def benchmark_resample(\n",
    "    method,\n",
    "    waveform,\n",
    "    sample_rate,\n",
    "    resample_rate,\n",
    "    lowpass_filter_width=DEFAULT_LOWPASS_FILTER_WIDTH,\n",
    "    rolloff=DEFAULT_ROLLOFF,\n",
    "    resampling_method=DEFAULT_RESAMPLING_METHOD,\n",
    "    beta=None,\n",
    "    librosa_type=None,\n",
    "    iters=5\n",
    "):\n",
    "  if method == \"functional\":\n",
    "    begin = time.time()\n",
    "    for _ in range(iters):\n",
    "      F.resample(waveform, sample_rate, resample_rate, lowpass_filter_width=lowpass_filter_width,\n",
    "                 rolloff=rolloff, resampling_method=resampling_method)\n",
    "    elapsed = time.time() - begin\n",
    "    return elapsed / iters\n",
    "  elif method == \"transforms\":\n",
    "    resampler = T.Resample(sample_rate, resample_rate, lowpass_filter_width=lowpass_filter_width,\n",
    "                           rolloff=rolloff, resampling_method=resampling_method, dtype=waveform.dtype)\n",
    "    begin = time.time()\n",
    "    for _ in range(iters):\n",
    "      resampler(waveform)\n",
    "    elapsed = time.time() - begin\n",
    "    return elapsed / iters\n",
    "  elif method == \"librosa\":\n",
    "    waveform_np = waveform.squeeze().numpy()\n",
    "    begin = time.time()\n",
    "    for _ in range(iters):\n",
    "      librosa.resample(waveform_np, sample_rate, resample_rate, res_type=librosa_type)\n",
    "    elapsed = time.time() - begin\n",
    "    return elapsed / iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7eb25951",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = get_spectrogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ac30c495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 201, 273])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7164290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = T.TimeMasking(time_mask_param=80)\n",
    "m2 = T.FrequencyMasking(freq_mask_param=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "002c63f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 201, 273])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1(spec).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4017f7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 201, 273])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2(spec).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5d571634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[5.4815e+00, 1.5772e-01, 1.3105e-01,  ..., 2.1306e+00,\n",
       "          6.8096e-01, 1.8927e-01],\n",
       "         [3.6053e+00, 5.9546e-02, 6.1161e-02,  ..., 5.8467e-01,\n",
       "          1.7731e-01, 2.2788e-02],\n",
       "         [8.1372e-01, 1.4541e-01, 1.4256e-01,  ..., 1.5266e-02,\n",
       "          6.5624e-03, 2.1952e-05],\n",
       "         ...,\n",
       "         [1.7497e-06, 7.0292e-07, 3.0349e-07,  ..., 2.7325e-07,\n",
       "          3.8198e-07, 1.1340e-06],\n",
       "         [6.9050e-06, 5.6465e-06, 1.2614e-06,  ..., 4.1065e-06,\n",
       "          4.2960e-06, 8.7536e-08],\n",
       "         [2.4680e-05, 1.9506e-05, 1.1039e-06,  ..., 4.9394e-06,\n",
       "          1.5377e-06, 8.1081e-07]]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "556d0f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(2,5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3e1c4246",
   "metadata": {},
   "outputs": [],
   "source": [
    "F_masking = T.FrequencyMasking(freq_mask_param=8)\n",
    "T_masking = T.TimeMasking(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "350e372d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5831,  0.7647,  0.5291, -1.0058,  0.0000,  0.0000,  2.6844,\n",
       "           1.6625, -0.9522, -0.4937],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 1.9215, -1.8687, -0.5001,  2.4065,  0.0000,  0.0000,  0.7449,\n",
       "          -0.0293,  2.2176, -0.3471]],\n",
       "\n",
       "        [[ 1.0196,  0.5147,  0.1018, -0.0246,  0.0000,  0.0000, -0.2270,\n",
       "           0.3002, -1.4887,  1.7888],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [-0.8963, -0.0539,  0.2602, -1.3655,  0.0000,  0.0000,  1.6722,\n",
       "           1.4802,  0.6824,  0.6240]]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input\n",
    "\n",
    "T_masking(F_masking(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d2634ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5831, -0.4303,  0.1760,  0.6151,  1.9215],\n",
       "         [ 0.7647, -1.3357,  0.3833, -0.6861, -1.8687],\n",
       "         [ 0.5291,  0.4817, -0.3189, -1.0730, -0.5001],\n",
       "         [-1.0058,  1.1554,  0.1707, -0.8427,  2.4065],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.9667, -0.1678, -1.4935, -0.4919, -1.2747],\n",
       "         [ 2.6844, -0.3962, -0.2095,  0.4335,  0.7449],\n",
       "         [ 1.6625, -0.3658, -0.8912, -1.1675, -0.0293],\n",
       "         [-0.9522,  0.1154, -0.4037,  0.3150,  2.2176],\n",
       "         [-0.4937,  0.8447,  0.6491, -0.5496, -0.3471]],\n",
       "\n",
       "        [[ 1.0196, -0.4688,  0.4398, -0.4461, -0.8963],\n",
       "         [ 0.5147, -0.6515,  0.5228, -0.8668, -0.0539],\n",
       "         [ 0.1018,  0.0149, -1.5218, -1.7143,  0.2602],\n",
       "         [-0.0246,  1.4833,  1.0476, -0.5805, -1.3655],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.2953,  0.6125, -0.1852,  0.6096,  1.0467],\n",
       "         [-0.2270, -0.1025, -0.4884, -1.0169,  1.6722],\n",
       "         [ 0.3002, -0.0715, -0.1779, -0.8892,  1.4802],\n",
       "         [-1.4887, -0.2890, -0.8684, -0.5427,  0.6824],\n",
       "         [ 1.7888,  1.5444, -0.6828, -0.9707,  0.6240]]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_masking(input.permute(0,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "69f3e047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000, -0.4303,  0.1760,  0.6151,  1.9215],\n",
       "         [ 0.0000, -1.3357,  0.3833, -0.6861, -1.8687],\n",
       "         [ 0.0000,  0.4817, -0.3189, -1.0730, -0.5001],\n",
       "         [ 0.0000,  1.1554,  0.1707, -0.8427,  2.4065],\n",
       "         [ 0.0000, -0.7807,  2.4633,  0.2499,  0.4161],\n",
       "         [ 0.0000, -0.1678, -1.4935, -0.4919, -1.2747],\n",
       "         [ 0.0000, -0.3962, -0.2095,  0.4335,  0.7449],\n",
       "         [ 0.0000, -0.3658, -0.8912, -1.1675, -0.0293],\n",
       "         [ 0.0000,  0.1154, -0.4037,  0.3150,  2.2176],\n",
       "         [ 0.0000,  0.8447,  0.6491, -0.5496, -0.3471]],\n",
       "\n",
       "        [[ 0.0000, -0.4688,  0.4398, -0.4461, -0.8963],\n",
       "         [ 0.0000, -0.6515,  0.5228, -0.8668, -0.0539],\n",
       "         [ 0.0000,  0.0149, -1.5218, -1.7143,  0.2602],\n",
       "         [ 0.0000,  1.4833,  1.0476, -0.5805, -1.3655],\n",
       "         [ 0.0000, -0.3169,  0.9239, -0.5607,  0.4776],\n",
       "         [ 0.0000,  0.6125, -0.1852,  0.6096,  1.0467],\n",
       "         [ 0.0000, -0.1025, -0.4884, -1.0169,  1.6722],\n",
       "         [ 0.0000, -0.0715, -0.1779, -0.8892,  1.4802],\n",
       "         [ 0.0000, -0.2890, -0.8684, -0.5427,  0.6824],\n",
       "         [ 0.0000,  1.5444, -0.6828, -0.9707,  0.6240]]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_masking(input.permute(0,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "727e23ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5831,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000, -0.9522, -0.4937],\n",
       "         [-0.4303,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.1154,  0.8447],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 1.9215,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  2.2176, -0.3471]],\n",
       "\n",
       "        [[ 1.0196,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000, -1.4887,  1.7888],\n",
       "         [-0.4688,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000, -0.2890,  1.5444],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [-0.8963,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.6824,  0.6240]]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = F_masking(T_masking(input.permute(0,2,1))).permute(0,2,1)\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e61e9cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5831,  0.7647,  0.5291, -1.0058,  1.4025,  0.0000,  0.0000,\n",
       "           1.6625, -0.9522, -0.4937],\n",
       "         [-0.4303, -1.3357,  0.4817,  1.1554, -0.7807,  0.0000,  0.0000,\n",
       "          -0.3658,  0.1154,  0.8447],\n",
       "         [ 0.1760,  0.3833, -0.3189,  0.1707,  2.4633,  0.0000,  0.0000,\n",
       "          -0.8912, -0.4037,  0.6491],\n",
       "         [ 0.6151, -0.6861, -1.0730, -0.8427,  0.2499,  0.0000,  0.0000,\n",
       "          -1.1675,  0.3150, -0.5496],\n",
       "         [ 1.9215, -1.8687, -0.5001,  2.4065,  0.4161,  0.0000,  0.0000,\n",
       "          -0.0293,  2.2176, -0.3471]],\n",
       "\n",
       "        [[ 1.0196,  0.5147,  0.1018, -0.0246, -0.7747,  0.0000,  0.0000,\n",
       "           0.3002, -1.4887,  1.7888],\n",
       "         [-0.4688, -0.6515,  0.0149,  1.4833, -0.3169,  0.0000,  0.0000,\n",
       "          -0.0715, -0.2890,  1.5444],\n",
       "         [ 0.4398,  0.5228, -1.5218,  1.0476,  0.9239,  0.0000,  0.0000,\n",
       "          -0.1779, -0.8684, -0.6828],\n",
       "         [-0.4461, -0.8668, -1.7143, -0.5805, -0.5607,  0.0000,  0.0000,\n",
       "          -0.8892, -0.5427, -0.9707],\n",
       "         [-0.8963, -0.0539,  0.2602, -1.3655,  0.4776,  0.0000,  0.0000,\n",
       "           1.4802,  0.6824,  0.6240]]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_masking(T_masking(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "31aa569e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = 10\n",
    "d = 5\n",
    "\n",
    "x = np.random.randn(d)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f5807bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = np.random.randn(h,1)\n",
    "n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "4e11918f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(n,x[None,:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "67d40058",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.random.randn(h,h)\n",
    "a = np.random.randn(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "58872a7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (5,) and (10,10) not aligned: 5 (dim 0) != 10 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [181]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (5,) and (10,10) not aligned: 5 (dim 0) != 10 (dim 0)"
     ]
    }
   ],
   "source": [
    "np.dot(x,b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ed793f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[None,:].squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "2e695b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "2661befa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0155,  0.3429, -0.3224, -0.0981,  0.3889],\n",
       "         [ 0.0437, -0.5859, -0.7280,  1.1164,  0.8519],\n",
       "         [ 0.8564, -1.0162, -0.5926,  0.7820, -2.4756],\n",
       "         [-0.2481,  0.7805, -0.9373,  0.6661,  1.3835],\n",
       "         [ 0.0712, -0.1261,  0.0105,  0.3130,  1.8059],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.0518, -0.1792,  1.2231, -2.0372,  0.4137],\n",
       "         [ 1.3483, -2.2431,  0.6741,  1.3352,  1.5516],\n",
       "         [ 1.4224,  1.6863,  0.5688,  0.4091,  0.4853],\n",
       "         [-1.2686, -0.0207, -1.2245, -2.1409,  0.4901]],\n",
       "\n",
       "        [[-1.8560,  0.0549, -0.2163,  1.8147, -0.6107],\n",
       "         [-0.0857, -0.2525, -0.0306,  0.9736,  0.1937],\n",
       "         [ 0.2665, -1.0867,  0.5624, -0.9791, -0.7055],\n",
       "         [-1.9815, -0.2526,  1.1718, -0.0612, -0.3958],\n",
       "         [-0.0612,  0.0043, -1.2013,  0.0409,  1.3253],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0481,  0.7593, -0.7225, -0.2812,  0.8025],\n",
       "         [ 1.1589, -2.3337, -0.8918,  1.4717,  0.6246],\n",
       "         [ 0.5953, -1.9792,  1.2224, -0.7995, -0.1560],\n",
       "         [ 1.9912, -0.7609,  1.4918,  0.8700, -1.1432]]])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_mfcc_pad = torch.randn(2,10,5)\n",
    "T_mask = torchaudio.transforms.TimeMasking(time_mask_param = batch_mfcc_pad.shape[1]//3)\n",
    "F_mask = torchaudio.transforms.FrequencyMasking(freq_mask_param = batch_mfcc_pad.shape[2]//3)\n",
    "batch_mfcc_pad = F_mask(T_mask(batch_mfcc_pad.permute(0,2,1))).permute(0,2,1)\n",
    "batch_mfcc_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "71965950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0155,  0.3429, -0.3224, -0.0981,  0.3889],\n",
       "         [ 0.0437, -0.5859, -0.7280,  1.1164,  0.8519],\n",
       "         [ 0.8564, -1.0162, -0.5926,  0.7820, -2.4756],\n",
       "         [-0.2481,  0.7805, -0.9373,  0.6661,  1.3835],\n",
       "         [ 0.0712, -0.1261,  0.0105,  0.3130,  1.8059],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.0518, -0.1792,  1.2231, -2.0372,  0.4137],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.4224,  1.6863,  0.5688,  0.4091,  0.4853],\n",
       "         [-1.2686, -0.0207, -1.2245, -2.1409,  0.4901]],\n",
       "\n",
       "        [[-1.8560,  0.0549, -0.2163,  1.8147, -0.6107],\n",
       "         [-0.0857, -0.2525, -0.0306,  0.9736,  0.1937],\n",
       "         [ 0.2665, -1.0867,  0.5624, -0.9791, -0.7055],\n",
       "         [-1.9815, -0.2526,  1.1718, -0.0612, -0.3958],\n",
       "         [-0.0612,  0.0043, -1.2013,  0.0409,  1.3253],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0481,  0.7593, -0.7225, -0.2812,  0.8025],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.5953, -1.9792,  1.2224, -0.7995, -0.1560],\n",
       "         [ 1.9912, -0.7609,  1.4918,  0.8700, -1.1432]]])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_mask(batch_mfcc_pad.permute(0,2,1)).permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "0476307c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torchaudio.transforms.FrequencyMasking(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "59fd4231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0155,  0.3429,  0.0000,  0.0000,  0.3889],\n",
       "         [ 0.0437, -0.5859,  0.0000,  0.0000,  0.8519],\n",
       "         [ 0.8564, -1.0162,  0.0000,  0.0000, -2.4756],\n",
       "         [-0.2481,  0.7805,  0.0000,  0.0000,  1.3835],\n",
       "         [ 0.0712, -0.1261,  0.0000,  0.0000,  1.8059],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.0518, -0.1792,  0.0000,  0.0000,  0.4137],\n",
       "         [ 1.3483, -2.2431,  0.0000,  0.0000,  1.5516],\n",
       "         [ 1.4224,  1.6863,  0.0000,  0.0000,  0.4853],\n",
       "         [-1.2686, -0.0207,  0.0000,  0.0000,  0.4901]],\n",
       "\n",
       "        [[-1.8560,  0.0549,  0.0000,  0.0000, -0.6107],\n",
       "         [-0.0857, -0.2525,  0.0000,  0.0000,  0.1937],\n",
       "         [ 0.2665, -1.0867,  0.0000,  0.0000, -0.7055],\n",
       "         [-1.9815, -0.2526,  0.0000,  0.0000, -0.3958],\n",
       "         [-0.0612,  0.0043,  0.0000,  0.0000,  1.3253],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0481,  0.7593,  0.0000,  0.0000,  0.8025],\n",
       "         [ 1.1589, -2.3337,  0.0000,  0.0000,  0.6246],\n",
       "         [ 0.5953, -1.9792,  0.0000,  0.0000, -0.1560],\n",
       "         [ 1.9912, -0.7609,  0.0000,  0.0000, -1.1432]]])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(batch_mfcc_pad.permute(0,2,1)).permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "1ba7944c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00, -1.3135e+00, -2.8134e+00,  1.7482e+00, -1.7523e+00],\n",
       "         [ 0.0000e+00,  2.2153e+00, -2.1999e-02,  2.7029e+00,  1.8583e-02],\n",
       "         [ 0.0000e+00,  1.1016e+00, -8.3178e-01, -2.7827e-02,  1.0546e+00],\n",
       "         [ 0.0000e+00,  4.6033e-01,  1.3721e+00,  4.8188e-01,  3.8821e-01],\n",
       "         [ 0.0000e+00, -5.2638e-01, -5.1887e-01, -1.5415e-01, -1.3531e-01],\n",
       "         [ 0.0000e+00,  9.4119e-01,  3.1419e-01, -2.0481e-03, -6.1116e-01],\n",
       "         [ 0.0000e+00, -1.7102e-01, -5.9784e-01, -9.7766e-01,  4.2888e-01],\n",
       "         [ 0.0000e+00, -1.1995e+00, -1.3188e+00, -1.5171e-01,  5.9850e-01]],\n",
       "\n",
       "        [[ 0.0000e+00,  5.3214e-01, -1.3193e+00, -3.8896e-01, -8.1950e-01],\n",
       "         [ 0.0000e+00,  1.1172e+00, -5.1964e-02,  7.2637e-01,  7.4349e-01],\n",
       "         [ 0.0000e+00, -7.6150e-01, -1.4637e-01, -1.6615e+00,  2.9489e-01],\n",
       "         [ 0.0000e+00,  1.1278e+00, -7.7399e-01,  7.3357e-01,  6.5055e-01],\n",
       "         [ 0.0000e+00, -1.2750e+00,  1.1723e+00, -1.4145e+00,  5.9107e-01],\n",
       "         [ 0.0000e+00,  1.5585e+00, -2.4039e-01,  1.2326e+00,  3.9635e-01],\n",
       "         [ 0.0000e+00, -2.2519e-01,  1.3156e-01, -2.7734e-02,  1.6239e+00],\n",
       "         [ 0.0000e+00,  1.6624e+00,  1.0603e-01,  1.3830e+00,  5.9594e-01]]])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2,8,5)\n",
    "f(x.permute(0,2,1)).permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "6276da51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "m = nn.Linear(10,20)\n",
    "optimizer = torch.optim.SGD(m.parameters(), lr = 0.1)\n",
    "s = CosineAnnealingWarmRestarts(optimizer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "c206c442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s) == CosineAnnealingWarmRestarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "4e955901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class New_Network(nn.Module):\n",
    "\n",
    "    def __init__(self, dims = None):\n",
    "        super(New_Network, self).__init__()\n",
    "        self.OUT_SIZE = 41\n",
    "\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv1d(15, 256, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.GELU(),\n",
    "\n",
    "            nn.Conv1d(256, 256, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = 256, hidden_size = 712, num_layers = 5, batch_first = True, dropout = 0.15, bidirectional = True)\n",
    "\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            #TODO: Linear layer with in_features from the lstm module above and out_features = OUT_SIZE\n",
    "            #nn.Linear(256, OUT_SIZE)\n",
    "            nn.Linear(1424, 1424),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.15),\n",
    "\n",
    "            nn.Linear(1424, self.OUT_SIZE)\n",
    "        )\n",
    "\n",
    "        self.logSoftmax = nn.LogSoftmax(dim = 2)\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x, lx):\n",
    "       out = x.permute(0,2,1)\n",
    "       out = self.backbone(out) \n",
    "       out = out.permute(0,2,1)\n",
    "       \n",
    "       out = pack_padded_sequence(out, lx.cpu().numpy(), batch_first=True, enforce_sorted=False)\n",
    "       out, (hn,cn) = self.lstm(out)\n",
    "       out, out_length = pad_packed_sequence(out, batch_first=True)\n",
    "\n",
    "       out = self.classifier(out)\n",
    "       out = self.logSoftmax(out)\n",
    "\n",
    "       return out, out_length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "6df8af8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "New_Network(\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv1d(15, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): GELU(approximate=none)\n",
       "    (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): GELU(approximate=none)\n",
       "  )\n",
       "  (lstm): LSTM(256, 712, num_layers=5, batch_first=True, dropout=0.15, bidirectional=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=1424, out_features=1424, bias=True)\n",
       "    (1): GELU(approximate=none)\n",
       "    (2): Dropout(p=0.15, inplace=False)\n",
       "    (3): Linear(in_features=1424, out_features=41, bias=True)\n",
       "  )\n",
       "  (logSoftmax): LogSoftmax(dim=2)\n",
       ")"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = New_Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "1a73a705",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2,1500,15)\n",
    "lx = torch.tensor([1000,1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "ed46350a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1500, 15])"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "e56ce207",
   "metadata": {},
   "outputs": [],
   "source": [
    "out, out_length = model(x,lx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "e6ad9ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1000, 41])"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "25fbd98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    # For this homework, we give you full flexibility to design your data set class.\n",
    "    # Hint: The data from HW1 is very similar to this HW\n",
    "\n",
    "    #TODO\n",
    "    def __init__(self, origin_path, train = True): \n",
    "        '''\n",
    "        Initializes the dataset.\n",
    "\n",
    "        INPUTS: What inputs do you need here?\n",
    "        '''\n",
    "        self.train = train\n",
    "        # Load the directory and all files in them\n",
    "        \n",
    "        self.origin_path = origin_path #\"/home/gyuseok/CMU/HW3/hw3p2/train-clean-360\"\n",
    "        \n",
    "        self.mfcc_dir = os.path.join(self.origin_path,\"mfcc\")\n",
    "        self.transcript_dir = os.path.join(self.origin_path, \"transcript\",\"raw\")\n",
    "\n",
    "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir)) #TODO\n",
    "        self.transcript_files = sorted(os.listdir(self.transcript_dir)) #TODO\n",
    "\n",
    "        self.PHONEMES = PHONEMES\n",
    "\n",
    "        #TODO\n",
    "        # WHAT SHOULD THE LENGTH OF THE DATASET BE?\n",
    "        self.length = len(self.mfcc_files)\n",
    "        \n",
    "        #TODO\n",
    "        # HOW CAN WE REPRESENT PHONEMES? CAN WE CREATE A MAPPING FOR THEM?\n",
    "        # HINT: TENSORS CANNOT STORE NON-NUMERICAL VALUES OR STRINGS\n",
    "        PHONEMES_dict = {letter:idx for idx,letter in enumerate(self.PHONEMES)}\n",
    "\n",
    "\n",
    "        #TODO\n",
    "        # CREATE AN ARRAY OF ALL FEATU ERS AND LABELS\n",
    "        # WHAT NORMALIZATION TECHNIQUE DID YOU USE IN HW1? CAN WE USE IT HERE?\n",
    "        self.mfccs = []\n",
    "        self.transcripts = []\n",
    "        \n",
    "        for i in range(self.length):\n",
    "            mfcc_path = os.path.join(self.mfcc_dir, self.mfcc_files[i])\n",
    "            label_path = os.path.join(self.transcript_dir, self.transcript_files[i])\n",
    "            \n",
    "            mfcc = np.load(mfcc_path)\n",
    "            label = np.load(label_path)\n",
    "            label = np.vectorize(PHONEMES_dict.get)(label) # transform into number\n",
    "            \n",
    "            self.mfccs.append(mfcc)\n",
    "            self.transcripts.append(label)\n",
    "        \n",
    "        '''\n",
    "        You may decide to do this in __getitem__ if you wish.\n",
    "        However, doing this here will make the __init__ function take the load of\n",
    "        loading the data, and shift it away from training.\n",
    "        '''\n",
    "       \n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        '''\n",
    "        TODO: What do we return here?\n",
    "        '''\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        '''\n",
    "        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n",
    "\n",
    "        If you didn't do the loading and processing of the data in __init__,\n",
    "        do that here.\n",
    "\n",
    "        Once done, return a tuple of features and labels.\n",
    "        '''\n",
    "        mfcc = self.mfccs[ind] # TODO\n",
    "        transcript = self.transcripts[ind] # TODO\n",
    "        \n",
    "        mfcc = torch.FloatTensor(mfcc)\n",
    "        transcript = torch.LongTensor(transcript)\n",
    "        return mfcc, transcript\n",
    "\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        '''\n",
    "        TODO:\n",
    "        1.  Extract the features and labels from 'batch'\n",
    "        2.  We will additionally need to pad both features and labels,\n",
    "            look at pytorch's docs for pad_sequence\n",
    "        3.  This is a good place to perform transforms, if you so wish. \n",
    "            Performing them on batches will speed the process up a bit.\n",
    "        4.  Return batch of features, labels, lenghts of features, \n",
    "            and lengths of labels.\n",
    "        '''\n",
    "        # batch of input mfcc coefficients\n",
    "        batch_mfcc = []\n",
    "        batch_transcript = []\n",
    "        lengths_mfcc = []\n",
    "        lengths_transcript = []\n",
    "        for x,y in batch:\n",
    "            batch_mfcc.append(x) # TODO\n",
    "            batch_transcript.append(y) # TODO\n",
    "            lengths_mfcc.append(x.shape[0])\n",
    "            lengths_transcript.append(y.shape[0])\n",
    "            \n",
    "        # HINT: CHECK OUT -> pad_sequence (imported above)\n",
    "        # Also be sure to check the input format (batch_first)\n",
    "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first = True) # TODO\n",
    "        batch_transcript_pad = pad_sequence(batch_transcript, batch_first = True) # TODO\n",
    "\n",
    "        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n",
    "        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n",
    "        #                  -> Would we apply transformation on the validation set as well?\n",
    "        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n",
    "        if self.train:\n",
    "            T_mask = torchaudio.transforms.TimeMasking(time_mask_param = 7)\n",
    "            F_mask = torchaudio.transforms.FrequencyMasking(freq_mask_param = batch_mfcc_pad.shape[1]//2)\n",
    "            batch_mfcc_pad = F_mask(T_mask(batch_mfcc_pad))\n",
    "        \n",
    "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
    "        return batch_mfcc_pad, batch_transcript_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "27df2c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd = os.getcwd()\n",
    "val_data = AudioDataset(os.path.join(pwd,\"hw3p2\",\"dev-clean\"), train = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "6bb88697",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AudioDataset' object has no attribute '__get__item'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [317]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mval_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get__item\u001b[49m(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AudioDataset' object has no attribute '__get__item'"
     ]
    }
   ],
   "source": [
    "val_data.__get__item(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88843e45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
