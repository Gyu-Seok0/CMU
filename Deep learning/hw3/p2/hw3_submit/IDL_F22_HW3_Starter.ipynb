{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UR4qfYrVoO4v"
   },
   "source": [
    "# Installs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rd5aNaLVoR_g"
   },
   "source": [
    "## wandb\n",
    "\n",
    "You will need to fetch your api key from wandb.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mA9qZoIDcx-h"
   },
   "outputs": [],
   "source": [
    "# !pip install wandb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PiDduMaDIARE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgyuseoklee\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/gyuseok/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"e0408f5d7b96be3d00be30b39eda0f1e259672ed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config[\"epochs\"] = 50\n",
    "config[\"project\"] = \"Transform\"\n",
    "config[\"model_save\"] = \"/home/gyuseok/CMU/IDL/HW3/weight\"\n",
    "config[\"batch_size\"] = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4s52yBOvICPZ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/gyuseok/CMU/IDL/HW3/wandb/run-20221119_102730-1oxgrwo1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/gyuseoklee/hw3p2-ablations/runs/1oxgrwo1\" target=\"_blank\">Transform</a></strong> to <a href=\"https://wandb.ai/gyuseoklee/hw3p2-ablations\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    name = config[\"project\"], ## Wandb creates random run names if you skip this field\n",
    "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
    "    # run_id = ### Insert specific run id here if you want to resume a previous run\n",
    "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
    "    project = \"hw3p2-ablations\", ### Project should be created in your wandb account \n",
    "    config = config ### Wandb Config for your run\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONgAWhqdoYy-"
   },
   "source": [
    "## Levenshtein\n",
    "\n",
    "This may take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SS7a7xeEoaV9"
   },
   "outputs": [],
   "source": [
    "# !pip install python-Levenshtein\n",
    "# !git clone --recursive https://github.com/parlance/ctcdecode.git\n",
    "# !pip install wget\n",
    "# %cd ctcdecode\n",
    "# !pip install .\n",
    "# %cd ..\n",
    "\n",
    "# !pip install torchsummaryX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWVONJxCobPc"
   },
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78ZTCIXoof2f",
    "outputId": "cf7c8f82-7aab-49ce-a68c-59b38e957cc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummaryX import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import torchaudio.transforms as tat\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import gc\n",
    "\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# imports for decoding and distance calculation\n",
    "import ctcdecode\n",
    "import Levenshtein\n",
    "from ctcdecode import CTCBeamDecoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gg3-yJ8tok34"
   },
   "source": [
    "# Kaggle Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AdUelfGhom1m"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
    "# #!mkdir /root/.kaggle\n",
    "\n",
    "# with open(\"/home/gyuseok/.kaggle/kaggle.json\", \"w+\") as f:\n",
    "#     f.write('{\"username\":\"leepro\",\"key\":\"2842f0b3a1e5a14ccddb8ba7fc0a016b\"}') \n",
    "\n",
    "# !chmod 600 /home/gyuseok/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dSjBwfXeoq4B",
    "outputId": "6515a9e6-3799-4293-d4b7-9e86bd61006c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !kaggle competitions download -c 11-785-f22-hw3p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ruxWP60LCQA",
    "outputId": "42a894c0-09fa-4650-8ad2-e75557ec7d6b"
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "# This will take a couple minutes, but you should see at least the following:\n",
    "# 11-785-f22-hw3p2.zip  ctcdecode  hw3p2\n",
    "# '''\n",
    "# !unzip -q 11-785-f22-hw3p2.zip\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9v5ewZDMpYA"
   },
   "source": [
    "# Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Cp-716IMZRd",
    "outputId": "ea6dfaa1-32bc-4f57-fde7-07f99056ed18"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ORNHnSFroP0"
   },
   "source": [
    "# Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "k0v7wHRWrqH6"
   },
   "outputs": [],
   "source": [
    "# ARPABET PHONEME MAPPING\n",
    "# DO NOT CHANGE\n",
    "# This overwrites the phonetics.py file.\n",
    "\n",
    "CMUdict_ARPAbet = {\n",
    "    \"\" : \" \", # BLANK TOKEN\n",
    "    \"[SIL]\": \"-\", \"NG\": \"G\", \"F\" : \"f\", \"M\" : \"m\", \"AE\": \"@\", \n",
    "    \"R\"    : \"r\", \"UW\": \"u\", \"N\" : \"n\", \"IY\": \"i\", \"AW\": \"W\", \n",
    "    \"V\"    : \"v\", \"UH\": \"U\", \"OW\": \"o\", \"AA\": \"a\", \"ER\": \"R\", \n",
    "    \"HH\"   : \"h\", \"Z\" : \"z\", \"K\" : \"k\", \"CH\": \"C\", \"W\" : \"w\", \n",
    "    \"EY\"   : \"e\", \"ZH\": \"Z\", \"T\" : \"t\", \"EH\": \"E\", \"Y\" : \"y\", \n",
    "    \"AH\"   : \"A\", \"B\" : \"b\", \"P\" : \"p\", \"TH\": \"T\", \"DH\": \"D\", \n",
    "    \"AO\"   : \"c\", \"G\" : \"g\", \"L\" : \"l\", \"JH\": \"j\", \"OY\": \"O\", \n",
    "    \"SH\"   : \"S\", \"D\" : \"d\", \"AY\": \"Y\", \"S\" : \"s\", \"IH\": \"I\",\n",
    "    \"[SOS]\": \"[SOS]\", \"[EOS]\": \"[EOS]\"}\n",
    "\n",
    "CMUdict = list(CMUdict_ARPAbet.keys())\n",
    "ARPAbet = list(CMUdict_ARPAbet.values())\n",
    "\n",
    "PHONEMES = CMUdict\n",
    "mapping = CMUdict_ARPAbet\n",
    "LABELS = ARPAbet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agmNBKf4JrLV"
   },
   "source": [
    "### Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "afd0_vlbJmr_"
   },
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    # For this homework, we give you full flexibility to design your data set class.\n",
    "    # Hint: The data from HW1 is very similar to this HW\n",
    "\n",
    "    #TODO\n",
    "    def __init__(self, origin_path, train = True): \n",
    "        '''\n",
    "        Initializes the dataset.\n",
    "\n",
    "        INPUTS: What inputs do you need here?\n",
    "        '''\n",
    "        self.train = train\n",
    "        # Load the directory and all files in them\n",
    "        \n",
    "        self.origin_path = origin_path #\"/home/gyuseok/CMU/HW3/hw3p2/train-clean-360\"\n",
    "        \n",
    "        self.mfcc_dir = os.path.join(self.origin_path,\"mfcc\")\n",
    "        self.transcript_dir = os.path.join(self.origin_path, \"transcript\",\"raw\")\n",
    "\n",
    "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir)) #TODO\n",
    "        self.transcript_files = sorted(os.listdir(self.transcript_dir)) #TODO\n",
    "\n",
    "        self.PHONEMES = PHONEMES\n",
    "\n",
    "        #TODO\n",
    "        # WHAT SHOULD THE LENGTH OF THE DATASET BE?\n",
    "        self.length = len(self.mfcc_files)\n",
    "        \n",
    "        #TODO\n",
    "        # HOW CAN WE REPRESENT PHONEMES? CAN WE CREATE A MAPPING FOR THEM?\n",
    "        # HINT: TENSORS CANNOT STORE NON-NUMERICAL VALUES OR STRINGS\n",
    "        PHONEMES_dict = {letter:idx for idx,letter in enumerate(self.PHONEMES)}\n",
    "\n",
    "\n",
    "        #TODO\n",
    "        # CREATE AN ARRAY OF ALL FEATU ERS AND LABELS\n",
    "        # WHAT NORMALIZATION TECHNIQUE DID YOU USE IN HW1? CAN WE USE IT HERE?\n",
    "        self.mfccs = []\n",
    "        self.transcripts = []\n",
    "        \n",
    "        for i in range(self.length):\n",
    "            mfcc_path = os.path.join(self.mfcc_dir, self.mfcc_files[i])\n",
    "            label_path = os.path.join(self.transcript_dir, self.transcript_files[i])\n",
    "            \n",
    "            mfcc = np.load(mfcc_path)\n",
    "            label = np.load(label_path)\n",
    "            label = np.vectorize(PHONEMES_dict.get)(label) # transform into number\n",
    "            \n",
    "            self.mfccs.append(mfcc)\n",
    "            self.transcripts.append(label)\n",
    "        \n",
    "        '''\n",
    "        You may decide to do this in __getitem__ if you wish.\n",
    "        However, doing this here will make the __init__ function take the load of\n",
    "        loading the data, and shift it away from training.\n",
    "        '''\n",
    "       \n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        '''\n",
    "        TODO: What do we return here?\n",
    "        '''\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        '''\n",
    "        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n",
    "\n",
    "        If you didn't do the loading and processing of the data in __init__,\n",
    "        do that here.\n",
    "\n",
    "        Once done, return a tuple of features and labels.\n",
    "        '''\n",
    "        mfcc = self.mfccs[ind] # TODO\n",
    "        transcript = self.transcripts[ind] # TODO\n",
    "        \n",
    "        mfcc = torch.FloatTensor(mfcc)\n",
    "        transcript = torch.LongTensor(transcript)\n",
    "        return mfcc, transcript\n",
    "\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        '''\n",
    "        TODO:\n",
    "        1.  Extract the features and labels from 'batch'\n",
    "        2.  We will additionally need to pad both features and labels,\n",
    "            look at pytorch's docs for pad_sequence\n",
    "        3.  This is a good place to perform transforms, if you so wish. \n",
    "            Performing them on batches will speed the process up a bit.\n",
    "        4.  Return batch of features, labels, lenghts of features, \n",
    "            and lengths of labels.\n",
    "        '''\n",
    "        # batch of input mfcc coefficients\n",
    "        batch_mfcc = []\n",
    "        batch_transcript = []\n",
    "        lengths_mfcc = []\n",
    "        lengths_transcript = []\n",
    "        for x,y in batch:\n",
    "            batch_mfcc.append(x) # TODO\n",
    "            batch_transcript.append(y) # TODO\n",
    "            lengths_mfcc.append(x.shape[0])\n",
    "            lengths_transcript.append(y.shape[0])\n",
    "            \n",
    "        # HINT: CHECK OUT -> pad_sequence (imported above)\n",
    "        # Also be sure to check the input format (batch_first)\n",
    "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first = True) # TODO\n",
    "        batch_transcript_pad = pad_sequence(batch_transcript, batch_first = True) # TODO\n",
    "\n",
    "        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n",
    "        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n",
    "        #                  -> Would we apply transformation on the validation set as well?\n",
    "        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n",
    "        if self.train:\n",
    "            T_mask = torchaudio.transforms.TimeMasking(time_mask_param = 7)\n",
    "            F_mask = torchaudio.transforms.FrequencyMasking(freq_mask_param = batch_mfcc_pad.shape[1]//2)\n",
    "            batch_mfcc_pad = F_mask(T_mask(batch_mfcc_pad))\n",
    "        \n",
    "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
    "        return batch_mfcc_pad, batch_transcript_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqDrxeHfJw4g"
   },
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "HrLS1wfVJppA"
   },
   "outputs": [],
   "source": [
    "# Test Dataloader\n",
    "#TODO\n",
    "class AudioDatasetTest(torch.utils.data.Dataset):\n",
    "    def __init__(self, origin_path): \n",
    "        '''\n",
    "        Initializes the dataset.\n",
    "\n",
    "        INPUTS: What inputs do you need here?\n",
    "        '''\n",
    "\n",
    "        # Load the directory and all files in them\n",
    "        self.origin_path = origin_path #\"/home/gyuseok/CMU/HW3/hw3p2/train-clean-360\"\n",
    "        self.mfcc_dir = os.path.join(self.origin_path,\"mfcc\")\n",
    "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir)) #TODO\n",
    "\n",
    "\n",
    "        #TODO\n",
    "        # WHAT SHOULD THE LENGTH OF THE DATASET BE?\n",
    "        self.length = len(self.mfcc_files)\n",
    "   \n",
    "        self.mfccs = []        \n",
    "        for i in range(self.length):\n",
    "            mfcc_path = os.path.join(self.mfcc_dir, self.mfcc_files[i])            \n",
    "            mfcc = np.load(mfcc_path)\n",
    "            self.mfccs.append(mfcc)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        mfcc = self.mfccs[ind] # TODO\n",
    "        mfcc = torch.FloatTensor(mfcc)\n",
    "        return mfcc\n",
    "\n",
    "\n",
    "    def collate_fn(self,batch):\n",
    "        '''\n",
    "        TODO:\n",
    "        1.  Extract the features and labels from 'batch'\n",
    "        2.  We will additionally need to pad both features and labels,\n",
    "            look at pytorch's docs for pad_sequence\n",
    "        3.  This is a good place to perform transforms, if you so wish. \n",
    "            Performing them on batches will speed the process up a bit.\n",
    "        4.  Return batch of features, labels, lenghts of features, \n",
    "            and lengths of labels.\n",
    "        '''\n",
    "        # batch of input mfcc coefficients\n",
    "        batch_mfcc = []\n",
    "        lengths_mfcc = []\n",
    "\n",
    "        for x in batch:\n",
    "            batch_mfcc.append(x) # TODO\n",
    "            lengths_mfcc.append(x.shape[0])\n",
    "\n",
    "            \n",
    "        # HINT: CHECK OUT -> pad_sequence (imported above)\n",
    "        # Also be sure to check the input format (batch_first)\n",
    "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first = True) # TODO\n",
    "\n",
    "        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n",
    "        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n",
    "        #                  -> Would we apply transformation on the validation set as well?\n",
    "        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n",
    "        \n",
    "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
    "        return batch_mfcc_pad, torch.tensor(lengths_mfcc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd = os.getcwd()\n",
    "train_data = AudioDataset(os.path.join(pwd, \"hw3p2\", \"train-clean-360\"), train = True) #train-clean-100\n",
    "val_data = AudioDataset(os.path.join(pwd,\"hw3p2\",\"dev-clean\"), train = False)\n",
    "test_data = AudioDatasetTest(os.path.join(pwd,\"hw3p2\",\"test-clean\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cepstral Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speechpy\n",
    "import numpy as np\n",
    "\n",
    "def concat_np(data: list):\n",
    "    return np.concatenate(data, axis = 0)\n",
    "\n",
    "def decompose_np(origin_data, target_data):\n",
    "    idxs = [0]\n",
    "    for i in range(len(origin_data)):\n",
    "        length = origin_data.mfccs[i].shape[0]\n",
    "        idxs.append(idxs[i] + length)\n",
    "    idxs = idxs[1:-1]\n",
    "    return np.split(target_data, idxs)\n",
    "    \n",
    "def noramlize_cmvn(train_data, val_data, test_data):\n",
    "    tr, val, te = concat_np(train_data.mfccs), concat_np(val_data.mfccs), concat_np(test_data.mfccs)\n",
    "    total_data = concat_np([tr, val, te])\n",
    "    total_data = speechpy.processing.cmvn(total_data, variance_normalization = True)\n",
    "    \n",
    "    train_idx = tr.shape[0]\n",
    "    test_idx = train_idx + val.shape[0]\n",
    "    \n",
    "    n_train = total_data[:train_idx, :]\n",
    "    n_val = total_data[train_idx:test_idx, :]\n",
    "    n_test = total_data[test_idx:, :]\n",
    "    \n",
    "    train_data.mfccs = decompose_np(train_data, n_train)\n",
    "    val_data.mfccs = decompose_np(val_data, n_val)\n",
    "    test_data.mfccs = decompose_np(test_data, n_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "noramlize_cmvn(train_data, val_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pt-veYcdL6Fe"
   },
   "source": [
    "### Data - Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "4icymeX1ImUN"
   },
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 32 # Increase if your device can handle it\n",
    "\n",
    "# transforms = [] # set of tranformations\n",
    "# # You may pass this as a parameter to the dataset class above\n",
    "# # This will help modularize your implementation\n",
    "\n",
    "# root = '/content/hw3p2' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmuPk9J6L8dz"
   },
   "source": [
    "### Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3_kG0gU2x4hH",
    "outputId": "95a65754-500e-42ba-99c8-7b90bd6e1ff4",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get me RAMMM!!!! \n",
    "import gc \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do NOT forget to pass in the collate function as parameter while creating the dataloader\n",
    "train_loader = DataLoader(train_data, batch_size = config[\"batch_size\"], \n",
    "                          shuffle = True, drop_last = False, collate_fn = train_data.collate_fn,\n",
    "                           pin_memory = True) #TODO\n",
    "\n",
    "val_loader = DataLoader(val_data, batch_size = config[\"batch_size\"], \n",
    "                        shuffle = False, drop_last = False, collate_fn = val_data.collate_fn,\n",
    "                         pin_memory = True)\n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size = config[\"batch_size\"], \n",
    "                         shuffle = False, drop_last = False, collate_fn = test_data.collate_fn,\n",
    "                          pin_memory = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  32\n",
      "Train dataset samples = 104014, batches = 3251\n",
      "Val dataset samples = 2703, batches = 85\n",
      "Test dataset samples = 2620, batches = 82\n"
     ]
    }
   ],
   "source": [
    "print(\"Batch size: \", config[\"batch_size\"])\n",
    "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
    "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
    "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1697, 15]) torch.Size([32, 201]) torch.Size([32]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "for data in train_loader:\n",
    "    x, y, lx, ly = data\n",
    "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ly4mjUUUuJhy"
   },
   "source": [
    "# Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RZ-qQ_Sf-LIu",
    "outputId": "ad9dc97d-1812-4bd4-cd13-50fed9d9034d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUT_SIZE = len(LABELS)\n",
    "OUT_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLad4pChcuvX"
   },
   "source": [
    "## Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "h_0 = Variable(torch.zeros(2, 3, 512), requires_grad = True)\n",
    "c_0 = Variable(torch.zeros(2, 3, 512), requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "EQhvHr71GJfq"
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from collections import Counter\n",
    "\n",
    "def DS_block(in_channel, out_channel, stride):\n",
    "    return nn.Sequential(\n",
    "                         nn.Conv1d(in_channel, in_channel, kernel_size = 3,  stride = stride, groups = in_channel, padding = 2),\n",
    "                         nn.BatchNorm1d(in_channel),\n",
    "                         nn.GELU(), #nn.ReLU(),\n",
    "        \n",
    "                         nn.Conv1d(in_channel, out_channel, kernel_size = 1, stride = 1),\n",
    "                         nn.BatchNorm1d(out_channel),\n",
    "                         nn.GELU() #nn.ReLU(),\n",
    "                        )\n",
    "\n",
    "def Res_Block(in_channel, out_channel):\n",
    "    return nn.Sequential(\n",
    "                         nn.Conv1d(in_channel, out_channel, kernel_size = 1, stride = 1),\n",
    "                         nn.BatchNorm1d(out_channel),\n",
    "                         nn.GELU(), #nn.ReLU(),\n",
    "        \n",
    "                         nn.Conv1d(out_channel, in_channel, kernel_size = 1, stride = 1),\n",
    "                         nn.BatchNorm1d(in_channel),\n",
    "                         nn.GELU(), #nn.ReLU(),\n",
    "                         )\n",
    "\n",
    "def make_DS_Res_Block(dims, strides):\n",
    "    ds_dims = dims[:-1]\n",
    "    rs_dims = dims[1:]\n",
    "\n",
    "    ds_layer = []\n",
    "    ds_dims = list(zip(ds_dims[:-1], ds_dims[1:]))\n",
    "    for idx, (in_dim, out_dim) in enumerate(ds_dims):\n",
    "        ds_layer += [DS_block(in_dim, out_dim, strides[idx])]\n",
    "\n",
    "    rs_layer = []\n",
    "    rs_dims = list(zip(rs_dims[:-1], rs_dims[1:]))\n",
    "    for in_dim, out_dim in rs_dims:\n",
    "        rs_layer += [Res_Block(in_dim, out_dim)]\n",
    "    \n",
    "    return ds_layer, rs_layer\n",
    "\n",
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self, dims = None):\n",
    "\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        # Adding some sort of embedding layer or feature extractor might help performance.\n",
    "        if dims is None:\n",
    "            #dims = [15, 1024, 1024, 1024, 1024, 1024, 512, 512, 512, 512, 512]\n",
    "            #dims = [15, 1024, 1024, 1024, 512, 512, 512, ]\n",
    "            #dims = [15, 512, 512, 512, 256, 256, 256, ]\n",
    "            self.dims = [15, 1024, 1024, 1024, 512, 512, 512]\n",
    "            self.strides = [2,1,1,2,1,1]\n",
    "        \n",
    "        self.lx_scale = 2 ** Counter(self.strides)[2]\n",
    "        \n",
    "        ds_layer, rs_layer = make_DS_Res_Block(self.dims, self.strides)\n",
    "        self.ds_layer = nn.ModuleList(ds_layer)\n",
    "        self.rs_layer = nn.ModuleList(rs_layer)\n",
    "\n",
    "        \n",
    "        # TODO : look up the documentation. You might need to pass some additional parameters.\n",
    "        #self.lstm1 = nn.LSTM(input_size = 256, hidden_size = 256, num_layers = 2, batch_first = True, dropout = 0.25) \n",
    "        #self.lstm2 = nn.LSTM(input_size = 256, hidden_size = 256, num_layers = 2, batch_first = True, dropout = 0.25)\n",
    "        \n",
    "        #self.lstm1 = nn.LSTM(input_size = 512, hidden_size = 256, num_layers = 2, batch_first = True, dropout = 0.25) \n",
    "        #self.lstm2 = nn.LSTM(input_size = 256, hidden_size = 256, num_layers = 2, batch_first = True, dropout = 0.25)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = 512, hidden_size = 256, num_layers = 3, batch_first = True, dropout = 0.25, bidirectional = True)\n",
    "        \n",
    "        \n",
    "        self.classification = nn.Sequential(\n",
    "            #TODO: Linear layer with in_features from the lstm module above and out_features = OUT_SIZE\n",
    "            #nn.Linear(256, OUT_SIZE)\n",
    "            nn.Linear(512, OUT_SIZE)\n",
    "        )\n",
    "\n",
    "        #TODO: Apply a log softmax here. Which dimension would apply it on ?\n",
    "\n",
    "        self.logSoftmax = nn.LogSoftmax(dim = 2)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x, lx):\n",
    "        out = x.permute(0,2,1)\n",
    "        for i in range(len(self.ds_layer)):\n",
    "            out = self.ds_layer[i](out)\n",
    "            out = self.rs_layer[i](out) + out\n",
    "        out = out.permute(0,2,1)\n",
    "        \n",
    "        #out = pack_padded_sequence(out, lx.cpu().numpy(), batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "#         out, (hn, cn) = self.lstm1(out)\n",
    "#         out, (hn, cn) = self.lstm2(out)\n",
    "        out, _ = self.lstm(out)\n",
    "        \n",
    "        #out, out_length = pad_packed_sequence(out, batch_first=True)\n",
    "        \n",
    "        out = self.classification(out)\n",
    "        out = self.logSoftmax(out)\n",
    "        \n",
    "        return out, lx // self.lx_scale  #out_length\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUThsowyQdN7"
   },
   "source": [
    "## INIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "CGoiXd70tb5z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================\n",
      "                                Kernel Shape     Output Shape    Params  \\\n",
      "Layer                                                                     \n",
      "0_ds_layer.0.Conv1d_0             [1, 15, 3]    [32, 15, 850]      60.0   \n",
      "1_ds_layer.0.BatchNorm1d_1              [15]    [32, 15, 850]      30.0   \n",
      "2_ds_layer.0.GELU_2                        -    [32, 15, 850]         -   \n",
      "3_ds_layer.0.Conv1d_3          [15, 1024, 1]  [32, 1024, 850]   16.384k   \n",
      "4_ds_layer.0.BatchNorm1d_4            [1024]  [32, 1024, 850]    2.048k   \n",
      "5_ds_layer.0.GELU_5                        -  [32, 1024, 850]         -   \n",
      "6_rs_layer.0.Conv1d_0        [1024, 1024, 1]  [32, 1024, 850]   1.0496M   \n",
      "7_rs_layer.0.BatchNorm1d_1            [1024]  [32, 1024, 850]    2.048k   \n",
      "8_rs_layer.0.GELU_2                        -  [32, 1024, 850]         -   \n",
      "9_rs_layer.0.Conv1d_3        [1024, 1024, 1]  [32, 1024, 850]   1.0496M   \n",
      "10_rs_layer.0.BatchNorm1d_4           [1024]  [32, 1024, 850]    2.048k   \n",
      "11_rs_layer.0.GELU_5                       -  [32, 1024, 850]         -   \n",
      "12_ds_layer.1.Conv1d_0          [1, 1024, 3]  [32, 1024, 852]    4.096k   \n",
      "13_ds_layer.1.BatchNorm1d_1           [1024]  [32, 1024, 852]    2.048k   \n",
      "14_ds_layer.1.GELU_2                       -  [32, 1024, 852]         -   \n",
      "15_ds_layer.1.Conv1d_3       [1024, 1024, 1]  [32, 1024, 852]   1.0496M   \n",
      "16_ds_layer.1.BatchNorm1d_4           [1024]  [32, 1024, 852]    2.048k   \n",
      "17_ds_layer.1.GELU_5                       -  [32, 1024, 852]         -   \n",
      "18_rs_layer.1.Conv1d_0       [1024, 1024, 1]  [32, 1024, 852]   1.0496M   \n",
      "19_rs_layer.1.BatchNorm1d_1           [1024]  [32, 1024, 852]    2.048k   \n",
      "20_rs_layer.1.GELU_2                       -  [32, 1024, 852]         -   \n",
      "21_rs_layer.1.Conv1d_3       [1024, 1024, 1]  [32, 1024, 852]   1.0496M   \n",
      "22_rs_layer.1.BatchNorm1d_4           [1024]  [32, 1024, 852]    2.048k   \n",
      "23_rs_layer.1.GELU_5                       -  [32, 1024, 852]         -   \n",
      "24_ds_layer.2.Conv1d_0          [1, 1024, 3]  [32, 1024, 854]    4.096k   \n",
      "25_ds_layer.2.BatchNorm1d_1           [1024]  [32, 1024, 854]    2.048k   \n",
      "26_ds_layer.2.GELU_2                       -  [32, 1024, 854]         -   \n",
      "27_ds_layer.2.Conv1d_3       [1024, 1024, 1]  [32, 1024, 854]   1.0496M   \n",
      "28_ds_layer.2.BatchNorm1d_4           [1024]  [32, 1024, 854]    2.048k   \n",
      "29_ds_layer.2.GELU_5                       -  [32, 1024, 854]         -   \n",
      "30_rs_layer.2.Conv1d_0        [1024, 512, 1]   [32, 512, 854]    524.8k   \n",
      "31_rs_layer.2.BatchNorm1d_1            [512]   [32, 512, 854]    1.024k   \n",
      "32_rs_layer.2.GELU_2                       -   [32, 512, 854]         -   \n",
      "33_rs_layer.2.Conv1d_3        [512, 1024, 1]  [32, 1024, 854]  525.312k   \n",
      "34_rs_layer.2.BatchNorm1d_4           [1024]  [32, 1024, 854]    2.048k   \n",
      "35_rs_layer.2.GELU_5                       -  [32, 1024, 854]         -   \n",
      "36_ds_layer.3.Conv1d_0          [1, 1024, 3]  [32, 1024, 428]    4.096k   \n",
      "37_ds_layer.3.BatchNorm1d_1           [1024]  [32, 1024, 428]    2.048k   \n",
      "38_ds_layer.3.GELU_2                       -  [32, 1024, 428]         -   \n",
      "39_ds_layer.3.Conv1d_3        [1024, 512, 1]   [32, 512, 428]    524.8k   \n",
      "40_ds_layer.3.BatchNorm1d_4            [512]   [32, 512, 428]    1.024k   \n",
      "41_ds_layer.3.GELU_5                       -   [32, 512, 428]         -   \n",
      "42_rs_layer.3.Conv1d_0         [512, 512, 1]   [32, 512, 428]  262.656k   \n",
      "43_rs_layer.3.BatchNorm1d_1            [512]   [32, 512, 428]    1.024k   \n",
      "44_rs_layer.3.GELU_2                       -   [32, 512, 428]         -   \n",
      "45_rs_layer.3.Conv1d_3         [512, 512, 1]   [32, 512, 428]  262.656k   \n",
      "46_rs_layer.3.BatchNorm1d_4            [512]   [32, 512, 428]    1.024k   \n",
      "47_rs_layer.3.GELU_5                       -   [32, 512, 428]         -   \n",
      "48_ds_layer.4.Conv1d_0           [1, 512, 3]   [32, 512, 430]    2.048k   \n",
      "49_ds_layer.4.BatchNorm1d_1            [512]   [32, 512, 430]    1.024k   \n",
      "50_ds_layer.4.GELU_2                       -   [32, 512, 430]         -   \n",
      "51_ds_layer.4.Conv1d_3         [512, 512, 1]   [32, 512, 430]  262.656k   \n",
      "52_ds_layer.4.BatchNorm1d_4            [512]   [32, 512, 430]    1.024k   \n",
      "53_ds_layer.4.GELU_5                       -   [32, 512, 430]         -   \n",
      "54_rs_layer.4.Conv1d_0         [512, 512, 1]   [32, 512, 430]  262.656k   \n",
      "55_rs_layer.4.BatchNorm1d_1            [512]   [32, 512, 430]    1.024k   \n",
      "56_rs_layer.4.GELU_2                       -   [32, 512, 430]         -   \n",
      "57_rs_layer.4.Conv1d_3         [512, 512, 1]   [32, 512, 430]  262.656k   \n",
      "58_rs_layer.4.BatchNorm1d_4            [512]   [32, 512, 430]    1.024k   \n",
      "59_rs_layer.4.GELU_5                       -   [32, 512, 430]         -   \n",
      "60_lstm                                    -   [32, 430, 512]  4.73088M   \n",
      "61_classification.Linear_0         [512, 43]    [32, 430, 43]   22.059k   \n",
      "62_logSoftmax                              -    [32, 430, 43]         -   \n",
      "\n",
      "                               Mult-Adds  \n",
      "Layer                                     \n",
      "0_ds_layer.0.Conv1d_0             38.25k  \n",
      "1_ds_layer.0.BatchNorm1d_1          15.0  \n",
      "2_ds_layer.0.GELU_2                    -  \n",
      "3_ds_layer.0.Conv1d_3            13.056M  \n",
      "4_ds_layer.0.BatchNorm1d_4        1.024k  \n",
      "5_ds_layer.0.GELU_5                    -  \n",
      "6_rs_layer.0.Conv1d_0          891.2896M  \n",
      "7_rs_layer.0.BatchNorm1d_1        1.024k  \n",
      "8_rs_layer.0.GELU_2                    -  \n",
      "9_rs_layer.0.Conv1d_3          891.2896M  \n",
      "10_rs_layer.0.BatchNorm1d_4       1.024k  \n",
      "11_rs_layer.0.GELU_5                   -  \n",
      "12_ds_layer.1.Conv1d_0         2.617344M  \n",
      "13_ds_layer.1.BatchNorm1d_1       1.024k  \n",
      "14_ds_layer.1.GELU_2                   -  \n",
      "15_ds_layer.1.Conv1d_3       893.386752M  \n",
      "16_ds_layer.1.BatchNorm1d_4       1.024k  \n",
      "17_ds_layer.1.GELU_5                   -  \n",
      "18_rs_layer.1.Conv1d_0       893.386752M  \n",
      "19_rs_layer.1.BatchNorm1d_1       1.024k  \n",
      "20_rs_layer.1.GELU_2                   -  \n",
      "21_rs_layer.1.Conv1d_3       893.386752M  \n",
      "22_rs_layer.1.BatchNorm1d_4       1.024k  \n",
      "23_rs_layer.1.GELU_5                   -  \n",
      "24_ds_layer.2.Conv1d_0         2.623488M  \n",
      "25_ds_layer.2.BatchNorm1d_1       1.024k  \n",
      "26_ds_layer.2.GELU_2                   -  \n",
      "27_ds_layer.2.Conv1d_3       895.483904M  \n",
      "28_ds_layer.2.BatchNorm1d_4       1.024k  \n",
      "29_ds_layer.2.GELU_5                   -  \n",
      "30_rs_layer.2.Conv1d_0       447.741952M  \n",
      "31_rs_layer.2.BatchNorm1d_1        512.0  \n",
      "32_rs_layer.2.GELU_2                   -  \n",
      "33_rs_layer.2.Conv1d_3       447.741952M  \n",
      "34_rs_layer.2.BatchNorm1d_4       1.024k  \n",
      "35_rs_layer.2.GELU_5                   -  \n",
      "36_ds_layer.3.Conv1d_0         1.314816M  \n",
      "37_ds_layer.3.BatchNorm1d_1       1.024k  \n",
      "38_ds_layer.3.GELU_2                   -  \n",
      "39_ds_layer.3.Conv1d_3       224.395264M  \n",
      "40_ds_layer.3.BatchNorm1d_4        512.0  \n",
      "41_ds_layer.3.GELU_5                   -  \n",
      "42_rs_layer.3.Conv1d_0       112.197632M  \n",
      "43_rs_layer.3.BatchNorm1d_1        512.0  \n",
      "44_rs_layer.3.GELU_2                   -  \n",
      "45_rs_layer.3.Conv1d_3       112.197632M  \n",
      "46_rs_layer.3.BatchNorm1d_4        512.0  \n",
      "47_rs_layer.3.GELU_5                   -  \n",
      "48_ds_layer.4.Conv1d_0           660.48k  \n",
      "49_ds_layer.4.BatchNorm1d_1        512.0  \n",
      "50_ds_layer.4.GELU_2                   -  \n",
      "51_ds_layer.4.Conv1d_3        112.72192M  \n",
      "52_ds_layer.4.BatchNorm1d_4        512.0  \n",
      "53_ds_layer.4.GELU_5                   -  \n",
      "54_rs_layer.4.Conv1d_0        112.72192M  \n",
      "55_rs_layer.4.BatchNorm1d_1        512.0  \n",
      "56_rs_layer.4.GELU_2                   -  \n",
      "57_rs_layer.4.Conv1d_3        112.72192M  \n",
      "58_rs_layer.4.BatchNorm1d_4        512.0  \n",
      "59_rs_layer.4.GELU_5                   -  \n",
      "60_lstm                        4.718592M  \n",
      "61_classification.Linear_0       22.016k  \n",
      "62_logSoftmax                          -  \n",
      "-------------------------------------------------------------------------------------\n",
      "                            Totals\n",
      "Total params            14.000261M\n",
      "Trainable params        14.000261M\n",
      "Non-trainable params           0.0\n",
      "Mult-Adds             7.065729913G\n",
      "=====================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kernel Shape</th>\n",
       "      <th>Output Shape</th>\n",
       "      <th>Params</th>\n",
       "      <th>Mult-Adds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0_ds_layer.0.Conv1d_0</th>\n",
       "      <td>[1, 15, 3]</td>\n",
       "      <td>[32, 15, 850]</td>\n",
       "      <td>60.0</td>\n",
       "      <td>38250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_ds_layer.0.BatchNorm1d_1</th>\n",
       "      <td>[15]</td>\n",
       "      <td>[32, 15, 850]</td>\n",
       "      <td>30.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_ds_layer.0.GELU_2</th>\n",
       "      <td>-</td>\n",
       "      <td>[32, 15, 850]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_ds_layer.0.Conv1d_3</th>\n",
       "      <td>[15, 1024, 1]</td>\n",
       "      <td>[32, 1024, 850]</td>\n",
       "      <td>16384.0</td>\n",
       "      <td>13056000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_ds_layer.0.BatchNorm1d_4</th>\n",
       "      <td>[1024]</td>\n",
       "      <td>[32, 1024, 850]</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>1024.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58_rs_layer.4.BatchNorm1d_4</th>\n",
       "      <td>[512]</td>\n",
       "      <td>[32, 512, 430]</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59_rs_layer.4.GELU_5</th>\n",
       "      <td>-</td>\n",
       "      <td>[32, 512, 430]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60_lstm</th>\n",
       "      <td>-</td>\n",
       "      <td>[32, 430, 512]</td>\n",
       "      <td>4730880.0</td>\n",
       "      <td>4718592.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61_classification.Linear_0</th>\n",
       "      <td>[512, 43]</td>\n",
       "      <td>[32, 430, 43]</td>\n",
       "      <td>22059.0</td>\n",
       "      <td>22016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62_logSoftmax</th>\n",
       "      <td>-</td>\n",
       "      <td>[32, 430, 43]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Kernel Shape     Output Shape     Params  \\\n",
       "Layer                                                                    \n",
       "0_ds_layer.0.Conv1d_0           [1, 15, 3]    [32, 15, 850]       60.0   \n",
       "1_ds_layer.0.BatchNorm1d_1            [15]    [32, 15, 850]       30.0   \n",
       "2_ds_layer.0.GELU_2                      -    [32, 15, 850]        NaN   \n",
       "3_ds_layer.0.Conv1d_3        [15, 1024, 1]  [32, 1024, 850]    16384.0   \n",
       "4_ds_layer.0.BatchNorm1d_4          [1024]  [32, 1024, 850]     2048.0   \n",
       "...                                    ...              ...        ...   \n",
       "58_rs_layer.4.BatchNorm1d_4          [512]   [32, 512, 430]     1024.0   \n",
       "59_rs_layer.4.GELU_5                     -   [32, 512, 430]        NaN   \n",
       "60_lstm                                  -   [32, 430, 512]  4730880.0   \n",
       "61_classification.Linear_0       [512, 43]    [32, 430, 43]    22059.0   \n",
       "62_logSoftmax                            -    [32, 430, 43]        NaN   \n",
       "\n",
       "                              Mult-Adds  \n",
       "Layer                                    \n",
       "0_ds_layer.0.Conv1d_0           38250.0  \n",
       "1_ds_layer.0.BatchNorm1d_1         15.0  \n",
       "2_ds_layer.0.GELU_2                 NaN  \n",
       "3_ds_layer.0.Conv1d_3        13056000.0  \n",
       "4_ds_layer.0.BatchNorm1d_4       1024.0  \n",
       "...                                 ...  \n",
       "58_rs_layer.4.BatchNorm1d_4       512.0  \n",
       "59_rs_layer.4.GELU_5                NaN  \n",
       "60_lstm                       4718592.0  \n",
       "61_classification.Linear_0      22016.0  \n",
       "62_logSoftmax                       NaN  \n",
       "\n",
       "[63 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = Network().to(device)\n",
    "#model = torch.nn.DataParallel(model, device_ids=[0, 1])\n",
    "\n",
    "summary(model, x.to(device), lx) # x and lx come from the sanity check above :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "out, out_lengths = model(x.to(device), lx)\n",
    "criterion = nn.CTCLoss()\n",
    "loss = criterion(out.permute(1,0,2), y, out_lengths, ly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.4160, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBwunYpyugFg"
   },
   "source": [
    "# Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "MN82c3KpLup8"
   },
   "outputs": [],
   "source": [
    "train_config = {\n",
    "    \"beam_width\" : 2,\n",
    "    \"lr\" : 2e-3,\n",
    "    \"epochs\" : 50\n",
    "    } # Feel free to add more items here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "iGoozH2nd6KB"
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "from torch.optim.lr_scheduler import StepLR \n",
    "\n",
    "criterion = nn.CTCLoss()\n",
    "\n",
    "# Define CTC loss as the criterion. How would the losses be reduced?\n",
    "# CTC Loss: https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html\n",
    "# Refer to the handout for hints\n",
    "\n",
    "optimizer =  torch.optim.AdamW(model.parameters(), lr = 0.002) # What goes in here?\n",
    "\n",
    "# Declare the decoder. Use the CTC Beam Decoder to decode phonemes\n",
    "# CTC Beam Decoder Doc: https://github.com/parlance/ctcdecode\n",
    "decoder = CTCBeamDecoder(\n",
    "                         labels = LABELS,\n",
    "                         beam_width = 5,\n",
    "                         num_processes = 40,\n",
    "                         log_probs_input = True\n",
    "                        ) #TODO \n",
    "\n",
    "scheduler = StepLR(optimizer, step_size = 30, gamma=0.1)#TODO\n",
    "\n",
    "# Mixed Precision, if you need it\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jmc6_4eWL2Xp"
   },
   "source": [
    "### Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "KHjnCDddL36E"
   },
   "outputs": [],
   "source": [
    "# Use debug = True to see debug outputs\n",
    "\n",
    "from Levenshtein import distance as lev\n",
    "\n",
    "def calculate_levenshtein(h, y, lh, ly, decoder, labels, debug = False):\n",
    "\n",
    "    if debug:\n",
    "        print(f\"\\n----- IN LEVENSHTEIN -----\\n\")\n",
    "        print(\"h\", h.shape)\n",
    "        print(\"y\", y.shape)\n",
    "        print(\"lh\", lh.shape)\n",
    "        print('ly', ly.shape)\n",
    "        # Add any other debug statements as you may need\n",
    "        # you may want to use debug in several places in this function\n",
    "        \n",
    "        \n",
    "    # TODO: look at docs for CTC.decoder and find out what is returned here\n",
    "    beam_results, beam_scores, timesteps, out_seq_len = decoder.decode(h, seq_lens = lh)\n",
    "    print(\"beam_scores\",beam_scores)\n",
    "    print(\"out_seq_len\",out_seq_len)\n",
    "\n",
    "    batch_size = beam_results.shape[0] # TODO\n",
    "    distance = 0 # Initialize the distance to be 0 initially\n",
    "\n",
    "    # TODO: Loop through each element in the batch\n",
    "    targets = []\n",
    "    for i in range(batch_size):\n",
    "        target = beam_results[i][0][:out_seq_len[i][0]]\n",
    "        t_string = \"\".join([LABELS[idx] for idx in target])\n",
    "        y_string = \"\".join([LABELS[idx] for idx in y[i][:ly[i]]])\n",
    "#         print(target, y[i][:ly[i]])\n",
    "#         distance += lev(target, y[i][:ly[i]])\n",
    "        #print(t_string, y_string)\n",
    "        distance += lev(t_string, y_string)\n",
    "        \n",
    "    \n",
    "\n",
    "    distance /= batch_size # TODO: Uncomment this, but think about why we are doing this\n",
    "\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- IN LEVENSHTEIN -----\n",
      "\n",
      "h torch.Size([32, 430, 43])\n",
      "y torch.Size([32, 201])\n",
      "lh torch.Size([32])\n",
      "ly torch.Size([32])\n",
      "beam_scores tensor([[1046.8346, 1046.8646, 1047.0068, 1047.6099, 1047.6266],\n",
      "        [1162.8406, 1163.1364, 1163.1370, 1163.1915, 1163.1938],\n",
      "        [1100.2616, 1100.2634, 1100.5109, 1100.8140, 1100.8335],\n",
      "        [ 884.6694,  885.5419,  885.5541,  885.5679,  885.5738],\n",
      "        [1081.1993, 1081.2040, 1082.0872, 1082.0890, 1082.0952],\n",
      "        [ 257.1790,  257.2653,  257.2874,  257.8599,  257.8665],\n",
      "        [1292.6403, 1292.6411, 1292.8605, 1292.8793, 1292.8993],\n",
      "        [1213.2466, 1213.2513, 1214.1342, 1214.1355, 1214.1389],\n",
      "        [1262.6636, 1262.6708, 1263.0203, 1263.0248, 1263.0250],\n",
      "        [1051.1140, 1051.1462, 1051.1501, 1051.8245, 1051.8301],\n",
      "        [1000.6113, 1000.6381, 1000.6389, 1001.1587, 1001.1763],\n",
      "        [1076.5792, 1076.8888, 1076.8916, 1076.9205, 1076.9214],\n",
      "        [1145.0446, 1145.0648, 1145.2179, 1145.2297, 1145.8020],\n",
      "        [1224.0615, 1224.0690, 1224.3695, 1224.3773, 1224.3901],\n",
      "        [ 723.7540,  723.7605,  723.7708,  723.8184,  724.4388],\n",
      "        [ 358.5529,  358.5650,  359.4174,  359.4274,  359.4568],\n",
      "        [ 790.9826,  791.7776,  791.7778,  791.7785,  791.7794],\n",
      "        [ 834.9300,  834.9518,  834.9560,  835.7323,  835.7379],\n",
      "        [1098.6180, 1098.9097, 1098.9286, 1099.4701, 1099.5028],\n",
      "        [1192.0159, 1192.0184, 1192.8787, 1192.9194, 1192.9250],\n",
      "        [ 997.9364,  997.9395,  997.9419,  997.9532,  998.8206],\n",
      "        [ 354.7779,  354.7923,  354.7949,  354.7998,  355.6618],\n",
      "        [1184.1399, 1184.1561, 1184.1561, 1184.1689, 1184.4650],\n",
      "        [1077.0223, 1077.0256, 1077.0385, 1077.7167, 1077.7203],\n",
      "        [1086.8167, 1087.0425, 1087.4492, 1087.4673, 1087.4825],\n",
      "        [1062.2816, 1062.2938, 1062.3054, 1062.3097, 1062.4178],\n",
      "        [1182.9601, 1183.2612, 1183.2655, 1183.2889, 1183.3044],\n",
      "        [ 324.5198,  324.5348,  324.7838,  324.7840,  325.0972],\n",
      "        [ 876.0115,  876.0281,  876.3032,  876.3217,  876.3428],\n",
      "        [ 897.6375,  897.6385,  898.2183,  898.2457,  898.4485],\n",
      "        [1213.7454, 1214.1530, 1214.1543, 1214.1549, 1214.1580],\n",
      "        [1232.7145, 1232.7156, 1232.7190, 1232.7202, 1232.7212]])\n",
      "out_seq_len tensor([[144, 144, 144, 145, 145],\n",
      "        [152, 151, 151, 151, 151],\n",
      "        [149, 149, 149, 148, 148],\n",
      "        [110, 109, 111, 111, 111],\n",
      "        [150, 150, 151, 151, 151],\n",
      "        [ 35,  36,  36,  36,  36],\n",
      "        [178, 178, 177, 177, 177],\n",
      "        [158, 158, 159, 159, 159],\n",
      "        [171, 171, 171, 171, 171],\n",
      "        [105, 105, 105, 106, 106],\n",
      "        [135, 135, 135, 135, 135],\n",
      "        [150, 149, 149, 149, 149],\n",
      "        [156, 156, 156, 156, 157],\n",
      "        [170, 170, 169, 169, 169],\n",
      "        [ 99,  99,  99,  99, 100],\n",
      "        [ 47,  47,  46,  46,  48],\n",
      "        [107, 108, 108, 108, 108],\n",
      "        [116, 116, 116, 117, 117],\n",
      "        [151, 151, 151, 150, 150],\n",
      "        [164, 164, 163, 165, 165],\n",
      "        [139, 139, 139, 139, 140],\n",
      "        [ 47,  47,  47,  47,  48],\n",
      "        [163, 163, 163, 163, 162],\n",
      "        [148, 148, 148, 149, 149],\n",
      "        [145, 145, 144, 144, 144],\n",
      "        [150, 150, 150, 150, 149],\n",
      "        [164, 163, 163, 163, 163],\n",
      "        [ 42,  42,  42,  42,  41],\n",
      "        [114, 114, 113, 113, 113],\n",
      "        [119, 119, 118, 118, 120],\n",
      "        [163, 162, 163, 162, 163],\n",
      "        [166, 165, 166, 166, 165]], dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "289.34375"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_levenshtein(out,y,out_lengths, ly, decoder, LABELS, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GnTLL-5gMBrY"
   },
   "outputs": [],
   "source": [
    "# ANOTEHR SANITY CHECK\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x, y, lx, ly = data\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        out, out_lengths = model(x, lx)\n",
    "        \n",
    "        loss = criterion(out.permute(1,0,2), y, out_lengths, ly).double().float()\n",
    "        \n",
    "      #TODO: \n",
    "      # Follow the following steps, and \n",
    "      # Add some print statements here for sanity checking\n",
    "      \n",
    "      #1. What values are you returning from the collate function\n",
    "\n",
    "      #2. Move the features and target to <DEVICE>\n",
    "    \n",
    "      #3. Print the shapes of each to get a fair understanding \n",
    "      #4. Pass the inputs to the model\n",
    "            # Think of the following before you implement:\n",
    "            # 4.1 What will be the input to your model?\n",
    "            # 4.2 What would the model output?\n",
    "            # 4.3 Print the shapes of the output to get a fair understanding \n",
    "\n",
    "      # Calculate loss: https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html\n",
    "      # Calculating the loss is not straightforward. Check the input format of each parameter\n",
    "        print(f\"loss: {loss}\")\n",
    "        distance = calculate_levenshtein(out, y, out_lengths, ly, decoder, LABELS, debug = False)\n",
    "        print(f\"lev-distance: {distance}\")\n",
    "        \n",
    "        \n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "\n",
    "        break # one iteration is enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fLLj5KIMMOe"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kH0RAbCaMl9a"
   },
   "source": [
    "### Eval function\n",
    "Writing a function to do one round of evaluations will help make your code more modular, you can, however, choose to skip this if you'd like it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nqLiAmkMMBc"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "def evaluate(data_loader, model, decoder, LABELS, device):\n",
    "    \n",
    "    dist = 0\n",
    "    total_loss = 0\n",
    "    batch_bar = tqdm(total=len(data_loader), dynamic_ncols=True, leave=False, position=0, desc='Val') \n",
    "    # TODO Fill this function out, if you're using it.\n",
    "    model.eval()\n",
    "    for x, y, lx, ly in data_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out, out_lengths = model(x, lx)\n",
    "            \n",
    "        loss = criterion(out.permute(1,0,2), y, out_lengths, ly)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        dist += calculate_levenshtein(out, y, out_lengths, ly,\n",
    "                                     decoder, LABELS)\n",
    "        \n",
    "    total_loss /= len(data_loader)\n",
    "    dist /= len(data_loader)\n",
    "    return total_loss, dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate(val_loader, model, decoder, LABELS, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpYExu4vT4_g"
   },
   "source": [
    "### Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tExvyl1BIdMC"
   },
   "outputs": [],
   "source": [
    "# This is for checkpointing, if you're doing it over multiple sessions\n",
    "# last_epoch_completed = 0\n",
    "# start = last_epoch_completed\n",
    "# end = epochs\n",
    "# dist_freq = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGn17rLw9ChF"
   },
   "source": [
    "Again, writing a train step might help you code be more modular. You may choose to skip this and write the whole thing out in the training loop below if you so wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vH4QStLUjH8"
   },
   "outputs": [],
   "source": [
    "def train_step(train_loader, model, optimizer, criterion, scheduler, device, scaler = None):\n",
    "    \n",
    "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
    "    train_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "\n",
    "        x, y, lx, ly = data\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # TODO: Fill this with the help of your sanity check\n",
    "        out, out_lengths = model(x, lx)\n",
    "\n",
    "        loss = criterion(out.permute(1,0,2), y, out_lengths, ly)\n",
    "\n",
    "        # HINT: Are you using mixed precision? \n",
    "\n",
    "        batch_bar.set_postfix(\n",
    "            loss = f\"{train_loss/ (i+1):.4f}\",\n",
    "            lr = f\"{optimizer.param_groups[0]['lr']}\"\n",
    "        )\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        batch_bar.update()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    batch_bar.close()\n",
    "    train_loss /= len(train_loader) # TODO\n",
    "\n",
    "    return train_loss # And anything else you may wish to get out of this function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MY69hgxUXhTI"
   },
   "source": [
    "### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JR43E28rM9Ak"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "#TODO: Please complete the training loop\n",
    "\n",
    "best_val_dist = float(\"inf\") # if you're restarting from some checkpoint, use what you saw there.\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    print(f\"epoch: {epoch}/{config['epochs']}\")\n",
    "    # one training step\n",
    "    train_loss = train_step(train_loader, model, optimizer, criterion, scheduler, device)\n",
    "\n",
    "    # one validation step (if you want)\n",
    "    val_loss, val_dist = evaluate(val_loader, model, decoder, LABELS, device)\n",
    "\n",
    "    # HINT: Calculating levenshtein distance takes a long time. Do you need to do it every epoch?\n",
    "    # Does the training step even need it? \n",
    "\n",
    "    # Where you have your scheduler.step depends on the scheduler you use.\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Use the below code to save models\n",
    "    if val_dist < best_val_dist:\n",
    "        print(\"Saving model\")\n",
    "        model_path = f\"{config['model_save']}/{config['project']}.pth\"\n",
    "\n",
    "        torch.save({'model_state_dict':model.state_dict(),\n",
    "                  'optimizer_state_dict':optimizer.state_dict(),\n",
    "                  'val_dist': val_dist, \n",
    "                  'epoch': epoch}, model_path)\n",
    "        best_val_dist = val_dist\n",
    "        wandb.save(model_path)\n",
    "    \n",
    "\n",
    "    # You may want to log some hyperparameters and results on wandb\n",
    "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
    "    wandb.log({\"train_loss\":train_loss, 'validation_loss': val_loss, \"val_dist\": val_dist, \n",
    "               \"learning_Rate\": curr_lr})\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(dic[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2H4EEj-sD32"
   },
   "source": [
    "# Generate Predictions and Submit to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2moYJhTWsOG-"
   },
   "outputs": [],
   "source": [
    "#TODO: Make predictions\n",
    "\n",
    "# Follow the steps below:\n",
    "# 1. Create a new object for CTCBeamDecoder with larger (why?) number of beams\n",
    "# 2. Get prediction string by decoding the results of the beam decoder\n",
    "\n",
    "decoder_test =  CTCBeamDecoder(\n",
    "                         labels = LABELS,\n",
    "                         beam_width = 20,\n",
    "                         num_processes = 40,\n",
    "                         log_probs_input = True\n",
    "                        ) \n",
    "\n",
    "def make_output(h, lh, decoder, LABELS):\n",
    "  \n",
    "    beam_results, beam_scores, timesteps, out_seq_len = decoder_test.decode(h, seq_lens = lh) #TODO: What parameters would the decode function take in?\n",
    "    batch_size = beam_results.shape[0] #What is the batch size\n",
    "\n",
    "    dist = 0\n",
    "    preds = []\n",
    "    for i in range(batch_size): # Loop through each element in the batch\n",
    "\n",
    "        h_sliced = beam_results[i][0][:out_seq_len[i][0]] #TODO: Obtain the beam results\n",
    "        h_string = \"\".join([LABELS[idx] for idx in h_sliced])#TODO: Convert the beam results to phonemes\n",
    "        preds.append(h_string)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d70dvu_lsMlv"
   },
   "outputs": [],
   "source": [
    "#TODO:\n",
    "# Write a function (predict) to generate predictions and submit the file to Kaggle\n",
    "def predict(test_loader, model, decoder, LABELS, device):\n",
    "    \n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for x, lx in test_loader:\n",
    "            x = x.to(device)\n",
    "            out, out_lengths = model(x, lx)\n",
    "            preds += make_output(out, out_lengths, decoder, LABELS)\n",
    "\n",
    "    return preds\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "predictions = predict(test_loader, model, decoder_test, LABELS, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = os.path.join(os.getcwd(),'hw3p2/test-clean/transcript/random_submission.csv')\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df.label = predictions\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'{config[\"project\"]}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kaggle competitions submit -c 11-785-f22-hw3p2 -f f'{config[\"project\"]}.csv' -m \"Message\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kaggle competitions submit -c 11-785-f22-hw3p2 -f f'{config[\"project\"]}.csv' -m \"Message\"\n",
    "command = f\"kaggle competitions submit -c 11-785-f22-hw3p2 -f {config['project']}.csv -m Message\"\n",
    "command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시도해보기\n",
    "\n",
    "<br> 1) Data Prallel -> Batch_True = False\n",
    "<br> 2) DownSampling -> CNN stride를 2이상으로 바꾸기 및 굳이 packed seq 쓰지 않기. -> 이때 output_lengts 대신 lx를 쓰면 된당\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions submit -c 11-785-f22-hw3p2 -f LSTM_3_layers.csv -m Message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions submit -c 11-785-f22-hw3p2 -f LSTM_3_layers_epoch31.csv -m Message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "UR4qfYrVoO4v",
    "gg3-yJ8tok34",
    "R9v5ewZDMpYA",
    "Ly4mjUUUuJhy",
    "HLad4pChcuvX",
    "tUThsowyQdN7",
    "IBwunYpyugFg",
    "kH0RAbCaMl9a",
    "qpYExu4vT4_g",
    "MY69hgxUXhTI",
    "M2H4EEj-sD32"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
