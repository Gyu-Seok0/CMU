{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Neural Machine Translation using Seq2Seq Models\n","\n","In HW06, we will be training our own machine translation system."],"metadata":{"id":"YId1X55ZPDWB"}},{"cell_type":"code","source":["!pip install torchmetrics"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MQy46o98kAfC","outputId":"81ba5e73-4b4e-42bf-b9be-a513af29845c","executionInfo":{"status":"ok","timestamp":1667926965900,"user_tz":300,"elapsed":10114,"user":{"displayName":"GyuSeok Lee","userId":"03967226578516930834"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchmetrics\n","  Downloading torchmetrics-0.10.2-py3-none-any.whl (529 kB)\n","\u001b[K     |████████████████████████████████| 529 kB 4.6 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.1+cu113)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n","Installing collected packages: torchmetrics\n","Successfully installed torchmetrics-0.10.2\n"]}]},{"cell_type":"markdown","source":["Mount your google drive here. We will be storing our training data in the google drive folder named `hw06_data`"],"metadata":{"id":"00jNulTjyG72"}},{"cell_type":"code","source":["from google.colab import drive\n","#drive._mount('/content/drive', force_remount=True)\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nlqHp24PgXNa","outputId":"e568dbbe-ceb1-4f4c-db88-a4221dfe4866","executionInfo":{"status":"ok","timestamp":1667931132481,"user_tz":300,"elapsed":1635,"user":{"displayName":"GyuSeok Lee","userId":"03967226578516930834"}}},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["Make sure you are using a GPU. Go to Runtime -> Change Runtime Type -> GPU.\n","\n","After running the below cell, if your notebook is configured to run on GPU, you should see a `device(type='cuda')` output"],"metadata":{"id":"lu9aGskp58pS"}},{"cell_type":"code","source":["from __future__ import unicode_literals, print_function, division\n","from io import open\n","import unicodedata\n","import string\n","import re\n","import random\n","\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","\n","import torchmetrics\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M2QfJZy3cygL","outputId":"4c903166-4170-43bf-eecc-87c5315ef143","executionInfo":{"status":"ok","timestamp":1667926985780,"user_tz":300,"elapsed":2349,"user":{"displayName":"GyuSeok Lee","userId":"03967226578516930834"}}},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["Object to store our language data. In this object, we assign a unique integer ID to every word in the vocab. We also assign new `<SOS>`, `<EOS>` and `<OOV>` tokens for start-of-sentence, end-of-sentence and out-of-vocab tokens respectively"],"metadata":{"id":"kFriOo4dyVC-"}},{"cell_type":"code","source":["SOS_token = 0\n","EOS_token = 1\n","OOV_token = 2\n","\n","\n","class Lang:\n","    def __init__(self, name):\n","        self.name = name\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"OOV\"}\n","        self.n_words = 3  # Count SOS, EOS and OOV\n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","        else:\n","            self.word2count[word] += 1"],"metadata":{"id":"1ltOnXrFfbWw","executionInfo":{"status":"ok","timestamp":1667926985780,"user_tz":300,"elapsed":5,"user":{"displayName":"GyuSeok Lee","userId":"03967226578516930834"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["Standard text pre-processing and string normalization. We convert unicode to ASCII and remove all punctuations"],"metadata":{"id":"QWx_Z9qmyo1V"}},{"cell_type":"code","source":["# Turn a Unicode string to plain ASCII, thanks to\n","# https://stackoverflow.com/a/518232/2809427\n","def unicodeToAscii(s):\n","    return ''.join(\n","        c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn'\n","    )\n","\n","# Lowercase, trim, and remove punctuations\n","def normalizeString(s):\n","    s = unicodeToAscii(s.lower().strip())\n","    s = s.translate(str.maketrans('', '', string.punctuation))\n","    return s"],"metadata":{"id":"GGfRHOH6feMG","executionInfo":{"status":"ok","timestamp":1667926985781,"user_tz":300,"elapsed":5,"user":{"displayName":"GyuSeok Lee","userId":"03967226578516930834"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def readLangs(lang1, lang2, path_to_corpus):\n","    print(\"Reading lines...\")\n","\n","    # Read the file and split into lines\n","    lines = open(path_to_corpus, encoding='utf-8').read().strip().split('\\n')\n","\n","    # Split every line into pairs and normalize\n","    pairs = [[normalizeString(s) for s in l.split('|||')] for l in lines]\n","        \n","    input_lang = Lang(lang1)\n","    output_lang = Lang(lang2)\n","\n","    return input_lang, output_lang, pairs"],"metadata":{"id":"TCzbE1VPfg2q","executionInfo":{"status":"ok","timestamp":1667926985781,"user_tz":300,"elapsed":4,"user":{"displayName":"GyuSeok Lee","userId":"03967226578516930834"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["We only consider sentences that are max 10 words in length. Think about why we might need to restrict the maximum number of words in a sentence."],"metadata":{"id":"gG06QhOJyxTS"}},{"cell_type":"code","source":["MAX_LENGTH = 10\n","\n","\n","def filterPair(p):\n","    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n","\n","def filterPairs(pairs):\n","    return [pair for pair in pairs if filterPair(pair)]"],"metadata":{"id":"6x3izAOAfsF1","executionInfo":{"status":"ok","timestamp":1667926985781,"user_tz":300,"elapsed":4,"user":{"displayName":"GyuSeok Lee","userId":"03967226578516930834"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Read in the data. Ensure that the path to the data files are correct"],"metadata":{"id":"AdCG4j2Oy-1b"}},{"cell_type":"code","source":["def prepareData(lang1, lang2, path_to_corpus):\n","    input_lang, output_lang, pairs = readLangs(lang1, lang2, path_to_corpus)\n","    print(\"Read %s sentence pairs\" % len(pairs))\n","    pairs = filterPairs(pairs) # limitation to max length 10\n","    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n","    print(\"Counting words...\")\n","    for pair in pairs:\n","        input_lang.addSentence(pair[0]) # english sentence\n","        output_lang.addSentence(pair[1]) # lang sentence\n","    print(\"Counted words:\")\n","    print(input_lang.name, input_lang.n_words)\n","    print(output_lang.name, output_lang.n_words)\n","    return input_lang, output_lang, pairs\n","\n","\n","# Give your input path here\n","HI_PATH = \"/content/gdrive/MyDrive/Colab Notebooks/hw06_data/eng_hin/ted-train.orig.eng-hin\"\n","ZH_PATH = \"/content/gdrive/MyDrive/Colab Notebooks/hw06_data/eng_zh/ted-train.orig.eng-zh\"\n","input_lang_train_en_hi, output_lang_train_hi, train_pairs_hi = prepareData('en', 'hi', HI_PATH)\n","input_lang_train_en_zh, output_lang_train_zh, train_pairs_zh = prepareData('en', 'zh', ZH_PATH)\n","print(random.choice(train_pairs_hi))\n","print(random.choice(train_pairs_zh))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4zTXiyj7fw-y","outputId":"e0ac178a-2f1d-499f-fbdd-71c0a8f4af36","executionInfo":{"status":"ok","timestamp":1667926990349,"user_tz":300,"elapsed":4410,"user":{"displayName":"GyuSeok Lee","userId":"03967226578516930834"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading lines...\n","Read 18798 sentence pairs\n","Trimmed to 4493 sentence pairs\n","Counting words...\n","Counted words:\n","en 4162\n","hi 4809\n","Reading lines...\n","Read 5534 sentence pairs\n","Trimmed to 2142 sentence pairs\n","Counting words...\n","Counted words:\n","en 2906\n","zh 2395\n","['that is a huge transformation', 'यह एक विशाल परिवरतन ह']\n","['does amazon carry it', '亚马逊？']\n"]}]},{"cell_type":"markdown","source":["Design your encoder model here"],"metadata":{"id":"8BVBkJVczD3D"}},{"cell_type":"code","source":["# Set your hidden size here\n","HIDDEN_SIZE = 256\n","\n","class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(EncoderRNN, self).__init__()\n","        # Assign hidden_size\n","        self.hidden_size = hidden_size ##YOUR CODE HERE\n","\n","        # Create nn.Embedding layer with (input_size, hidden_size)\n","        self.embedding = nn.Embedding(input_size, hidden_size) ##YOUR CODE HERE\n","\n","        # Create nn.GRU layer with (hidden_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size, 1)##YOUR CODE HERE\n","\n","    def forward(self, input, hidden):\n","        # Run the input through the embedding layer\n","        embedded = self.embedding(input) ##YOUR CODE HERE: torch.Size([1, 256])\n","\n","        # Reshape with (1, 1, -1)\n","        embedded = embedded.reshape(1, 1, -1) ##YOUR CODE HERE: torch.Size([1, 1, 256])\n","\n","\n","        # Run both the embedded and hidden through GRU\n","        output, hidden = self.gru(embedded, hidden) ##YOUR CODE HERE: (1, 1, HIDDEN_SIZE), (1, 1, HIDDEN_SIZE)\n","\n","        # Return both output and hidden\n","        return output, hidden\n","\n","    def initHidden(self):\n","        # Create a torch tensor of zeros of shape (1, 1, HIDDEN_SIZE)\n","        return torch.zeros(1,1,self.hidden_size).to(device) ##YOUR CODE HERE\n","\n","dummy_in = torch.randint(1, 10, (1,), device=device) # tensor([7], device='cuda:0')\n","dummy_encoder = EncoderRNN(100, HIDDEN_SIZE).to(device) # \n","assert dummy_encoder.initHidden().shape == (1, 1, HIDDEN_SIZE) # init hidden\n","dummy_out, dummy_hid = dummy_encoder.forward(dummy_in, dummy_encoder.initHidden())\n","assert dummy_out.shape == (1, 1, HIDDEN_SIZE)\n","assert dummy_hid.shape == (1, 1, HIDDEN_SIZE)"],"metadata":{"id":"rk_1ZuLWj5N9","executionInfo":{"status":"ok","timestamp":1667926998199,"user_tz":300,"elapsed":3615,"user":{"displayName":"GyuSeok Lee","userId":"03967226578516930834"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["Design your decoder model here"],"metadata":{"id":"5ekxl5MczGpy"}},{"cell_type":"code","source":["class DecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, output_size):\n","        super(DecoderRNN, self).__init__()\n","        # Assign hidden_size\n","        self.hidden_size = hidden_size ##YOUR CODE HERE\n","\n","        # Create nn.Embedding layer with (output_size, hidden_size)\n","        self.embedding = nn.Embedding(output_size, hidden_size) ##YOUR CODE HERE\n","\n","        # Create nn.GRU layer with (hidden_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size, 1) ##YOUR CODE HERE\n","\n","        # Create a nn.Linear layer with (hidden_size, output_size)\n","        self.out = nn.Linear(hidden_size, output_size) ##YOUR CODE HERE\n","\n","        # Create a nn.LogSoftmax layer with (dim=1)\n","        self.softmax = nn.LogSoftmax(dim = 1) ##YOUR CODE HERE\n","\n","    def forward(self, input, hidden):\n","        # Run the input through the embedding layer\n","        input = self.embedding(input) ##YOUR CODE HERE\n","\n","        # Reshape the input with (1, 1, -1)\n","        input = input.reshape(1,1,-1) ##YOUR CODE HERE\n","\n","        # Use relu activation \n","        input = F.relu(input)\n","\n","        # Run both the input and hidden through GRU\n","        output, hidden = self.gru(input, hidden) ##YOUR CODE HERE\n","        \n","        # Reshape the output with (1, -1)\n","        output = output.reshape(1,-1) ##YOUR CODE HERE\n","\n","        # Run the output through the linear layer\n","        output = self.out(output) ##YOUR CODE HERE\n","\n","        # Get softmax scores\n","        output = self.softmax(output) ##YOUR CODE HERE\n","\n","        # Return both output and hidden\n","        return output, hidden\n","\n","    def initHidden(self):\n","        # Create a torch tensor of zeros of shape (1, 1, HIDDEN_SIZE)\n","        return torch.zeros(1,1,self.hidden_size).to(device) ##YOUR CODE HERE\n","\n","dummy_in = torch.randint(1, 10, (1,), device=device)\n","dummy_decoder = DecoderRNN(HIDDEN_SIZE, 100).to(device)\n","assert dummy_decoder.initHidden().shape == (1, 1, HIDDEN_SIZE)\n","dummy_out, dummy_hid = dummy_decoder.forward(dummy_in, dummy_decoder.initHidden())\n","assert dummy_out.shape == (1, 100)\n","assert dummy_hid.shape == (1, 1, HIDDEN_SIZE)"],"metadata":{"id":"lxuXcv_4kgLk","executionInfo":{"status":"ok","timestamp":1667926998200,"user_tz":300,"elapsed":17,"user":{"displayName":"GyuSeok Lee","userId":"03967226578516930834"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["Helper functions to convert the sentences into vector. Given a sentence, we convert it into a vector of word IDs"],"metadata":{"id":"eaYxxOpPzJII"}},{"cell_type":"code","source":["def indexesFromSentence(lang, sentence):\n","    return [lang.word2index.get(word, OOV_token) for word in sentence.split(' ')]\n","\n","def tensorFromSentence(lang, sentence):\n","    indexes = indexesFromSentence(lang, sentence)\n","    indexes.append(EOS_token)\n","    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n","\n","def tensorsFromPair(input_lang, output_lang, pair):\n","    input_tensor = tensorFromSentence(input_lang, pair[0])\n","    target_tensor = tensorFromSentence(output_lang, pair[1])\n","    return (input_tensor, target_tensor)\n"],"metadata":{"id":"XWLLb5nPk2bh","executionInfo":{"status":"ok","timestamp":1667926998200,"user_tz":300,"elapsed":16,"user":{"displayName":"GyuSeok Lee","userId":"03967226578516930834"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["Implement the main training loop here. This function takes in one input tensor and one output tensor and does a forward pass, backward pass and weight updates. "],"metadata":{"id":"TJ9VcIcgzStu"}},{"cell_type":"code","source":["teacher_forcing_ratio = 0.5\n","\n","\n","def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, only_forward_pass=False):\n","    # Initialize the hidden layer for encoder\n","    encoder_hidden = encoder.initHidden() ##YOUR CODE HERE\n","\n","    # Reset gradients for both encoder_optimizer and decoder_optimizer\n","    encoder_optimizer.zero_grad() ##YOUR CODE HERE\n","    decoder_optimizer.zero_grad() ##YOUR CODE HERE\n","\n","    input_length = input_tensor.size(0)\n","    target_length = target_tensor.size(0)\n","\n","    loss = 0\n","\n","    for ei in range(input_length):\n","        # Run each input word through the encoder\n","        # You can access the current input as input_tensor[ei]\n","        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden) ##YOUR CODE HERE)\n","\n","    # First input to the decoder\n","    decoder_input = torch.tensor([[SOS_token]], device=device)\n","\n","    # Assign the last encoder_hidden to decoder_hidden\n","    decoder_hidden = encoder_hidden ##YOUR CODE HERE\n","\n","    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","\n","    if use_teacher_forcing:\n","        # Teacher forcing: Feed the target as the next input\n","        for di in range(target_length):\n","            # Run decoder by providing decoder_input and decoder_hidden as input\n","            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden) ##YOUR CODE HERE)\n","\n","            # Calculate loss\n","            loss += criterion(decoder_output, target_tensor[di])\n","            decoder_input = target_tensor[di]  # Teacher forcing\n","\n","    else:\n","        # Without teacher forcing: use its own predictions as the next input\n","        for di in range(target_length):\n","            # Run decoder by providing decoder_input and decoder_hidden as input\n","            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden) ##YOUR CODE HERE)\n","\n","            # Take the top output of current timestep of decoder. This will be input to next timestep\n","            topv, topi = decoder_output.topk(1)\n","            decoder_input = topi.squeeze().detach()  # detach from history as input\n","\n","            loss += criterion(decoder_output, target_tensor[di])\n","            if decoder_input.item() == EOS_token:\n","                break\n","\n","    if not only_forward_pass:\n","        # Backprop by calling backward() function on loss\n","        loss.backward() ##YOUR CODE HERE\n","\n","        # Update weights using step() on both encoder_optimizer and decoder_optimizer\n","        encoder_optimizer.step()##YOUR CODE HERE\n","        decoder_optimizer.step() ##YOUR CODE HERE\n","\n","    return loss.item() / target_length"],"metadata":{"id":"_N-h0dp0k8wo","executionInfo":{"status":"ok","timestamp":1667926998201,"user_tz":300,"elapsed":16,"user":{"displayName":"GyuSeok Lee","userId":"03967226578516930834"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["def trainIters(encoder, decoder, n_iters, input_lang, output_lang, pairs, learning_rate=0.01):\n","    # Initialize SGD optimizers for both encoder and decoder\n","    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n","    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n","    \n","    # Convert words to tensors\n","    all_training_pairs = [tensorsFromPair(input_lang, output_lang, pair) for pair in pairs]\n","\n","    # Create training and valid datasets\n","    random.shuffle(all_training_pairs)\n","    valid_pairs = all_training_pairs[:500]\n","    training_pairs = all_training_pairs[500:]\n","    \n","    # We will be using NLLLoss as criterion\n","    criterion = nn.NLLLoss()\n","\n","    epoch_train_losses = []\n","    epoch_valid_losses = []\n","\n","    # In each epoch, we go through all training examples\n","    for iter in range(1, n_iters + 1):\n","\n","        # Train\n","        train_loss = 0.0\n","        for training_pair in training_pairs:\n","            input_tensor = training_pair[0]\n","            target_tensor = training_pair[1]\n","\n","            loss = train(input_tensor, target_tensor, encoder,\n","                      decoder, encoder_optimizer, decoder_optimizer, criterion)\n","            train_loss += loss\n","\n","        # Validate\n","        valid_loss = 0.0\n","        for val_pair in valid_pairs:\n","            input_tensor = val_pair[0]\n","            output_tensor = val_pair[1]\n","            loss = train(input_tensor, target_tensor, encoder,\n","                      decoder, encoder_optimizer, decoder_optimizer, criterion, only_forward_pass=True)\n","            valid_loss += loss\n","\n","        avg_train_loss = train_loss / len(training_pairs)\n","        avg_valid_loss = valid_loss / len(valid_pairs)\n","\n","        print(\"Epoch: {}/{}. Avg Train Loss: {}. Avg Valid Loss: {}\".format(iter, n_iters, avg_train_loss, avg_valid_loss))\n","\n","        epoch_train_losses.append(avg_train_loss)\n","        epoch_valid_losses.append(avg_valid_loss)\n","\n","    return epoch_train_losses, epoch_valid_losses"],"metadata":{"id":"0xV9QrE_lGyb","executionInfo":{"status":"ok","timestamp":1667926998506,"user_tz":300,"elapsed":321,"user":{"displayName":"GyuSeok Lee","userId":"03967226578516930834"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["Train the model. What do you notice about the training and validation losses?"],"metadata":{"id":"iLCVURlPzlUb"}},{"cell_type":"code","source":["# ENG - HIN\n","encoder_eng_hi = EncoderRNN(input_lang_train_en_hi.n_words, HIDDEN_SIZE).to(device)\n","decoder_eng_hi = DecoderRNN(HIDDEN_SIZE, output_lang_train_hi.n_words).to(device)\n","\n","avg_train_losses_hi, avg_valid_losses_hi = trainIters(encoder_eng_hi, decoder_eng_hi, 30, input_lang_train_en_hi, output_lang_train_hi, train_pairs_hi)\n","\n","# ENG - ZH\n","encoder_eng_zh = EncoderRNN(input_lang_train_en_zh.n_words, HIDDEN_SIZE).to(device)\n","decoder_eng_zh = DecoderRNN(HIDDEN_SIZE, output_lang_train_zh.n_words).to(device)\n","\n","avg_train_losses_zh, avg_valid_losses_zh = trainIters(encoder_eng_zh, decoder_eng_zh, 30, input_lang_train_en_zh, output_lang_train_zh, train_pairs_zh)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"adWQ2vUWry_5","executionInfo":{"status":"ok","timestamp":1667928258345,"user_tz":300,"elapsed":1259842,"user":{"displayName":"GyuSeok Lee","userId":"03967226578516930834"}},"outputId":"e06c4513-fe72-4e79-d02e-766b7f21cdc8"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/30. Avg Train Loss: 4.8587927698139115. Avg Valid Loss: 3.7179735960279188\n","Epoch: 2/30. Avg Train Loss: 4.64201456489197. Avg Valid Loss: 4.016932969229561\n","Epoch: 3/30. Avg Train Loss: 4.390332147195442. Avg Valid Loss: 4.432969999313357\n","Epoch: 4/30. Avg Train Loss: 4.105104902559589. Avg Valid Loss: 4.698872936793735\n","Epoch: 5/30. Avg Train Loss: 3.7589293600371527. Avg Valid Loss: 4.988428626741683\n","Epoch: 6/30. Avg Train Loss: 3.389894002306302. Avg Valid Loss: 5.469907428332741\n","Epoch: 7/30. Avg Train Loss: 3.0211862115990535. Avg Valid Loss: 5.56451514993395\n","Epoch: 8/30. Avg Train Loss: 2.6542840060736843. Avg Valid Loss: 5.63467682729449\n","Epoch: 9/30. Avg Train Loss: 2.3218260580239214. Avg Valid Loss: 6.184317752429413\n","Epoch: 10/30. Avg Train Loss: 1.9795294545420634. Avg Valid Loss: 6.225307368006029\n","Epoch: 11/30. Avg Train Loss: 1.6640282825671135. Avg Valid Loss: 6.332162562234056\n","Epoch: 12/30. Avg Train Loss: 1.4003537360370304. Avg Valid Loss: 6.780699701036727\n","Epoch: 13/30. Avg Train Loss: 1.1426607749294166. Avg Valid Loss: 7.116554161071777\n","Epoch: 14/30. Avg Train Loss: 0.9297670755974268. Avg Valid Loss: 7.17284124428885\n","Epoch: 15/30. Avg Train Loss: 0.7727923403008792. Avg Valid Loss: 7.628442907605848\n","Epoch: 16/30. Avg Train Loss: 0.6399395560609595. Avg Valid Loss: 7.34171572385515\n","Epoch: 17/30. Avg Train Loss: 0.5201218513910737. Avg Valid Loss: 7.518793613978788\n","Epoch: 18/30. Avg Train Loss: 0.40653672156141213. Avg Valid Loss: 7.529518105643135\n","Epoch: 19/30. Avg Train Loss: 0.3190428500838571. Avg Valid Loss: 7.420894730704162\n","Epoch: 20/30. Avg Train Loss: 0.2384869684934532. Avg Valid Loss: 7.672590088435586\n","Epoch: 21/30. Avg Train Loss: 0.1826802672384406. Avg Valid Loss: 7.737934936795922\n","Epoch: 22/30. Avg Train Loss: 0.14971272307739475. Avg Valid Loss: 7.875791304452078\n","Epoch: 23/30. Avg Train Loss: 0.13424122962728488. Avg Valid Loss: 7.9474105954851355\n","Epoch: 24/30. Avg Train Loss: 0.12296632561858856. Avg Valid Loss: 7.906037096296033\n","Epoch: 25/30. Avg Train Loss: 0.1171225496629999. Avg Valid Loss: 8.158047776086\n","Epoch: 26/30. Avg Train Loss: 0.11302253746719636. Avg Valid Loss: 8.161198857988635\n","Epoch: 27/30. Avg Train Loss: 0.10716736356842485. Avg Valid Loss: 8.329356577736988\n","Epoch: 28/30. Avg Train Loss: 0.10271662119795155. Avg Valid Loss: 8.244236153193876\n","Epoch: 29/30. Avg Train Loss: 0.10116824834919706. Avg Valid Loss: 8.384775435311452\n","Epoch: 30/30. Avg Train Loss: 0.09977947678475071. Avg Valid Loss: 8.31031737354824\n","Epoch: 1/30. Avg Train Loss: 4.1848629832412945. Avg Valid Loss: 5.55097578811645\n","Epoch: 2/30. Avg Train Loss: 4.091286030578699. Avg Valid Loss: 5.471216277440393\n","Epoch: 3/30. Avg Train Loss: 3.9642482043988454. Avg Valid Loss: 5.319925237973533\n","Epoch: 4/30. Avg Train Loss: 3.7978634794474466. Avg Valid Loss: 5.38697821331024\n","Epoch: 5/30. Avg Train Loss: 3.596859911016414. Avg Valid Loss: 5.37133131154378\n","Epoch: 6/30. Avg Train Loss: 3.3410054927122363. Avg Valid Loss: 5.402749087651577\n","Epoch: 7/30. Avg Train Loss: 3.062616296182113. Avg Valid Loss: 5.473201470692953\n","Epoch: 8/30. Avg Train Loss: 2.7227022549392337. Avg Valid Loss: 5.644020699818935\n","Epoch: 9/30. Avg Train Loss: 2.354215790609589. Avg Valid Loss: 5.693612410863242\n","Epoch: 10/30. Avg Train Loss: 1.974418396196829. Avg Valid Loss: 5.756817562421158\n","Epoch: 11/30. Avg Train Loss: 1.5967772511417484. Avg Valid Loss: 5.938123127301537\n","Epoch: 12/30. Avg Train Loss: 1.2204768118389187. Avg Valid Loss: 6.042686388333642\n","Epoch: 13/30. Avg Train Loss: 0.9022513981124708. Avg Valid Loss: 6.048252049763996\n","Epoch: 14/30. Avg Train Loss: 0.6562847300658738. Avg Valid Loss: 6.294196272214262\n","Epoch: 15/30. Avg Train Loss: 0.5020218106942994. Avg Valid Loss: 6.4147777153650924\n","Epoch: 16/30. Avg Train Loss: 0.401635901266471. Avg Valid Loss: 6.673058091481526\n","Epoch: 17/30. Avg Train Loss: 0.3325884590899836. Avg Valid Loss: 6.824538475672397\n","Epoch: 18/30. Avg Train Loss: 0.2877622286088831. Avg Valid Loss: 6.936366022109985\n","Epoch: 19/30. Avg Train Loss: 0.24970589353080191. Avg Valid Loss: 7.077130605061846\n","Epoch: 20/30. Avg Train Loss: 0.2214068563522675. Avg Valid Loss: 7.162773475646975\n","Epoch: 21/30. Avg Train Loss: 0.20005598529640903. Avg Valid Loss: 7.273416444142658\n","Epoch: 22/30. Avg Train Loss: 0.18484543981166712. Avg Valid Loss: 7.517244660059615\n","Epoch: 23/30. Avg Train Loss: 0.16894391729548228. Avg Valid Loss: 7.509521277745565\n","Epoch: 24/30. Avg Train Loss: 0.15762892212674093. Avg Valid Loss: 7.607826869010926\n","Epoch: 25/30. Avg Train Loss: 0.14492337440227138. Avg Valid Loss: 7.769609590212507\n","Epoch: 26/30. Avg Train Loss: 0.13656341400348365. Avg Valid Loss: 7.766301115671792\n","Epoch: 27/30. Avg Train Loss: 0.1287962665329897. Avg Valid Loss: 7.990572221755986\n","Epoch: 28/30. Avg Train Loss: 0.12158128256053853. Avg Valid Loss: 8.001272415796917\n","Epoch: 29/30. Avg Train Loss: 0.11765123139282253. Avg Valid Loss: 8.113530842463165\n","Epoch: 30/30. Avg Train Loss: 0.11252494091058389. Avg Valid Loss: 8.179585200627649\n"]}]},{"cell_type":"code","source":["# # ENG - HIN\n","# encoder_eng_hi = EncoderRNN(input_lang_train_en_hi.n_words, HIDDEN_SIZE).to(device)\n","# decoder_eng_hi = DecoderRNN(HIDDEN_SIZE, output_lang_train_hi.n_words).to(device)\n","\n","# avg_train_losses_hi, avg_valid_losses_hi = trainIters(encoder_eng_hi, decoder_eng_hi, 30, input_lang_train_en_hi, output_lang_train_hi, train_pairs_hi)\n","\n","# # ENG - ZH\n","# encoder_eng_zh = EncoderRNN(input_lang_train_en_zh.n_words, HIDDEN_SIZE).to(device)\n","# decoder_eng_zh = DecoderRNN(HIDDEN_SIZE, output_lang_train_zh.n_words).to(device)\n","\n","# avg_train_losses_zh, avg_valid_losses_zh = trainIters(encoder_eng_zh, decoder_eng_zh, 30, input_lang_train_en_zh, output_lang_train_zh, train_pairs_zh)"],"metadata":{"id":"y3580qu0lQT0","executionInfo":{"status":"ok","timestamp":1667928258346,"user_tz":300,"elapsed":20,"user":{"displayName":"GyuSeok Lee","userId":"03967226578516930834"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fFqUoWwPS6B1","executionInfo":{"status":"ok","timestamp":1667928665346,"user_tz":300,"elapsed":152,"user":{"displayName":"GyuSeok Lee","userId":"03967226578516930834"}},"outputId":"0a4b0808-5245-4e2a-f2cb-0ee0de0e2b69"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["with open(\"/content/gdrive/MyDrive/Colab Notebooks/losses_hi.txt\", \"w\") as fp:\n","    for i,j in zip(avg_train_losses_hi, avg_valid_losses_hi):\n","      fp.write(\"{:.12f} {:.12f}\\n\".format(i, j))\n","\n","with open(\"/content/gdrive/MyDrive/Colab Notebooks/losses_zh.txt\", \"w\") as fp:\n","    for i,j in zip(avg_train_losses_zh, avg_valid_losses_zh):\n","      fp.write(\"{:.12f} {:.12f}\\n\".format(i, j))"],"metadata":{"id":"lmizO_aSKjDd","executionInfo":{"status":"ok","timestamp":1667928736473,"user_tz":300,"elapsed":180,"user":{"displayName":"GyuSeok Lee","userId":"03967226578516930834"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["Function to perform inference. Given an input sentence, this function returns the translated sentence"],"metadata":{"id":"1ehweWG2ztnl"}},{"cell_type":"code","source":["def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n","    with torch.no_grad():\n","        input_tensor = tensorFromSentence(input_lang, sentence)\n","        input_length = input_tensor.size()[0]\n","        encoder_hidden = encoder.initHidden()\n","\n","        for ei in range(input_length):\n","            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n","\n","        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n","\n","        decoder_hidden = encoder_hidden\n","\n","        decoded_words = []\n","\n","        for di in range(MAX_LENGTH):\n","            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n","            \n","            topv, topi = decoder_output.data.topk(1)\n","            if topi.item() == EOS_token:\n","                decoded_words.append('<EOS>')\n","                break\n","            else:\n","                decoded_words.append(output_lang.index2word[topi.item()])\n","\n","            decoder_input = topi.squeeze().detach()\n","\n","        return decoded_words"],"metadata":{"id":"AX1gbuhRgC2a","executionInfo":{"status":"ok","timestamp":1667928355392,"user_tz":300,"elapsed":131,"user":{"displayName":"GyuSeok Lee","userId":"03967226578516930834"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["Try out a random input. \n","\n","1. Does the output make sense if given an input from the training set?\n","2. Does the output make sense if given an arbitrary input?"],"metadata":{"id":"yN6VGxL4z3JY"}},{"cell_type":"code","source":["print(evaluate(encoder_eng_hi, decoder_eng_hi, \"we can go into the water\", input_lang_train_en_hi, output_lang_train_hi))\n","\n","print(evaluate(encoder_eng_zh, decoder_eng_zh, \"Five new flagship stores opened.\", input_lang_train_en_zh, output_lang_train_zh))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tge_5xmLrSkn","outputId":"1ce97000-93c7-4d17-884e-d8cbff225213","executionInfo":{"status":"ok","timestamp":1667928258347,"user_tz":300,"elapsed":6,"user":{"displayName":"GyuSeok Lee","userId":"03967226578516930834"}}},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["['पानी', 'पानी', 'को', 'पानी', 'जा', 'सकत', 'ह।', '<EOS>']\n","['我意思係話，癌症點睇', '<EOS>']\n"]}]},{"cell_type":"markdown","source":["Calculate CHRF score"],"metadata":{"id":"BxzpFaKQ0EjN"}},{"cell_type":"code","source":["hyp_hi = []\n","ref_hi = []\n","\n","for sample in train_pairs_hi:\n","    x = sample[0]\n","    y_true = sample[1]\n","    y_pred_tokens = evaluate(encoder_eng_hi, decoder_eng_hi, x, input_lang_train_en_hi, output_lang_train_hi)\n","\n","    if \"<EOS>\" in y_pred_tokens:\n","        y_pred_tokens.remove(\"<EOS>\")\n","    y_pred = \" \".join(y_pred_tokens)\n","\n","    hyp_hi.append(y_pred)\n","    ref_hi.append([y_true])\n","\n","hyp_zh = []\n","ref_zh = []\n","\n","for sample in train_pairs_zh:\n","    x = sample[0]\n","    y_true = sample[1]\n","    y_pred_tokens = evaluate(encoder_eng_zh, decoder_eng_zh, x, input_lang_train_en_zh, output_lang_train_zh)\n","\n","    if \"<EOS>\" in y_pred_tokens:\n","        y_pred_tokens.remove(\"<EOS>\")\n","    y_pred = \" \".join(y_pred_tokens)\n","\n","    hyp_zh.append(y_pred)\n","    ref_zh.append([y_true])\n","\n","metric = torchmetrics.CHRFScore()\n","print(\"Hindi CHRF Score\", metric(hyp_hi, ref_hi))\n","print(\"Chinese CHRF Score\", metric(hyp_zh, ref_zh))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q7vE5rlDhsFx","outputId":"6a8ac65c-e15c-4ee9-f12b-d339e9fe320f","executionInfo":{"status":"ok","timestamp":1667928327643,"user_tz":300,"elapsed":69301,"user":{"displayName":"GyuSeok Lee","userId":"03967226578516930834"}}},"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchmetrics/functional/text/chrf.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  total_n_grams[n] = tensor(sum(n_grams_counts[n].values()))\n","/usr/local/lib/python3.7/dist-packages/torchmetrics/functional/text/chrf.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  for n_gram in hyp_n_grams_counts[n]\n"]},{"output_type":"stream","name":"stdout","text":["Hindi CHRF Score tensor(0.8743)\n","Chinese CHRF Score tensor(0.8134)\n"]}]},{"cell_type":"code","source":["with open(\"/content/gdrive/MyDrive/Colab Notebooks/predictions_hi.txt\", \"w\") as fp:\n","    for i,j in zip(hyp_hi, ref_hi):\n","        fp.write(\"{} {}\\n\".format(i, str(j)))\n","\n","with open(\"/content/gdrive/MyDrive/Colab Notebooks/predictions_zh.txt\", \"w\") as fp:\n","    for i,j in zip(hyp_zh, ref_zh):\n","        fp.write(\"{} {}\\n\".format(i, str(j)))"],"metadata":{"id":"mYKcTsdU-2X9","executionInfo":{"status":"ok","timestamp":1667928778482,"user_tz":300,"elapsed":116,"user":{"displayName":"GyuSeok Lee","userId":"03967226578516930834"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TjVdXSR2LwxU","executionInfo":{"status":"ok","timestamp":1667928327644,"user_tz":300,"elapsed":18,"user":{"displayName":"GyuSeok Lee","userId":"03967226578516930834"}}},"execution_count":26,"outputs":[]}]}